	ref_title	ref_context	ref_entry	abstract	intro	label	retrieval_result
0	Ai in literature reviews a survey of current and emerging methods		"Zongyue Li, Xiaofei Lu ( B ) , Jing Chen, Haishan Wang, Xu Wang, Qinghui Shi, Dejun Xue, Yanhong Bi, and Zixuan Huang

CNKI Large Model and Future Technology R&D Department,Tongfang Knowledge Network Technology Co., Ltd., Beijing, China { lzy14922,lxf5511,cj11678,whs7417,wx9018,shiqinghui, xuedejun,byh11630,hzx15069 } @cnki.net"	large language models (llms) have revolutionized natural language processing (nlp) tasks, yet they struggle with handling long text inputs due to token limitations. this paper introduces a novel method for creating ai-driven literature surveys by integrating clustering techniques with a modiﬁed tf-idf formula (c-tf-idf). our approach organizes documents into clusters, enabling the production of comprehensive and concise literature surveys. this method not only overcomes token limitations but also improves the relevance and coverage of the generated surveys. experimental results indicate that our approach signiﬁcantly outperforms traditional llm methods in both coverage and informativeness. keywords: clustering analysis · retrieval-augmented generation · ai-driven scientiﬁc research	in scientiﬁc research, literature surveys are essential for summarizing and synthesizing existing knowledge, identifying research gaps, and guiding future studies. with the rapid increase of scientiﬁc publications, researchers face the challenge of eﬃciently managing and integrating extensive volumes of information. artiﬁcial intelligence (ai), particularly oﬀ-the-shelf large language models(llms), such as gpt-3 [ 1 ], chatgpt and gpt-4 [ 2 ], have shown great potential in automating the generation of literature surveys, thereby alleviating the burden on researchers’ time and resources. nonetheless, these models do have their own limitations. one of the major challenges in using llms for generating literature surveys is their tendency to produce hallucinations, creating information that is not grounded in the source material. this issue was notably observed with the galactica system [ 3 ], which exhibited excessive hallucinations in literature survey tasks, ultimately leading to its discontinuation. additionally, llms often face token limitations, restricting the amount of input they can process at once, which can hinder their performance and result in incomplete summaries. recent advancements, notably the retrieval-augmented generation (rag) approach [ 4 ], have addressed some of these issues by integrating external knowledge sources [ 5 ]. while rag can mitigate hallucinations and enhance factual accuracy, it also poses challenges in eﬃciently selecting and recalling relevant content without overwhelming the model with excessive tokens [ 6 ]. therefore, there is a need for innovative solutions to handle large sets of references and generate structured and coherent outlines for literature surveys. to tackle these issues, this paper proposes a structured, clustering-based method aimed at enhancing the accuracy and eﬃciency of ai-driven literature surveys generation. our approach, illustrated in fig. 1 , consists of four key stages: clustering analysis: embedding and clustering references based on speciﬁc subjects to organize the content into manageable sections. rag with llm: employing retrieval-augmented generation (rag) alongside large language models (llms) to formulate headings, subheadings and contextualize each section. section integration: compiling the individually generated sections into an initial literature survey to create a comprehensive document that addresses various aspects of the research topic. llm optimization: iteratively reﬁning the initial survey using llms to generate abstracts, conclusions and titles, ensuring clarity, coherence, and logical ﬂow. fig. 1. workﬂow of the proposed clustering-based method for ai-generated literature surveys. in summary, the main contributions of the paper are as follows: novel framework proposal: we introduce a new framework that uses clustering techniques to manage and organize references in ai-generated literature surveys. eﬀectiveness evaluation: we demonstrate the eﬀectiveness of this clustering method through participation in the scientiﬁc literature survey generation competition (nlpcc 2024 shared task 6) [ 7 ]. our approach achieved superior scores in metrics such as soft heading recall and rouge, surpassing those of other teams and demonstrating its eﬀectiveness in overcoming token limitations and enhancing overall content management. practical application: we examine the potential applications of this method across various scientiﬁc ﬁelds and emphasize its practical signiﬁcance for advancing future research. these contributions collectively enhance the structure and coherence of aidriven literature surveys, addressing token limitations and improving overall content management.	0	The methods mentioned in the context include the Adaptive-RAG framework, which dynamically selects the most suitable retrieval strategy for question answering based on query complexity, and DyLAN, a dynamic framework that selects and organizes LLM agents for tasks such as reasoning and code generation using an inference-time selection mechanism and an agent team optimization algorithm. These methods aim to enhance the efficiency and effectiveness of AI tools in academic research.
1	Autosurvey large language models can automatically write surveys		"1 st Mohamed Saied School of information technology and computer science Nile university Giza, Egypt M.Abdelnaser2159@nu.edu.eg

2 nd Nada Mokhtar School of information technology and computer science Nile university Giza, Egypt N.Hussain2128@nu.edu.eg

3 rd Abdelrahman Badr

School of information technology and computer science

4 th Michael Adel

School of information technology and computer science Nile university Giza, Egypt M.Hany2160@nu.edu.eg

5 th Philopater Boles

School of information technology and computer science

5 th Ghada Khoriba

School of information technology and computer science"	ai-assisted literature review tools have significantly enhanced researchers’ productivity by streamlining the review process and reducing time consumption. although traditional tools remain widely used, a new generation of tools leveraging state-of-the-art methods, including large language models (llms), is gaining popularity. however, llms face challenges such as hallucinations, which affect their reliability and accuracy. to address this, solutions such as knowledge augmentation are being explored. additionally, combining knowledge-augmented llms with agentic frameworks has shown promise in improving their performance, making them more reliable and effective for literature review tasks. index terms —ai-assisted literature review, large language models, llm hallucinations, knowledge augmentation, agentic frameworks.	knowledge, and identifies potential avenues for future investigation. however, as the volume of scholarly publications expands exponentially, researchers face increasing challenges in keeping up with the sheer quantity of new information. manually curating, synthesizing, and critically analyzing various academic sources has become increasingly time-consuming and labor-intensive. consequently, researchers and institutions have begun exploring ai-based tools and techniques to automate and streamline the literature review process. automation of literature reviews often referred to as ”aiassisted literature reviews,” involves using machine learning algorithms, natural language processing (nlp), and other ai technologies to assist or entirely replace human involvement in certain stages of the review process. these technologies offer several potential benefits, including faster processing times, increased scalability, enhanced reproducibility, and reduced human bias. by leveraging ai tools, researchers can gather, sort, and synthesize relevant academic works more efficiently, enabling them to focus on higher-order cognitive tasks such as critical analysis and interpretation. ai in automating literature reviews is still an emerging area of research, but it has shown promise in various applications. these include systematic literature reviews (slrs), metaanalyses, scoping reviews, and narrative reviews across multiple disciplines. ai systems are being developed and refined to perform automatic citation extraction, document classification, topic modeling, summarization, sentiment analysis, and network mapping functions. machine learning models can also be trained to identify key concepts, themes, and relationships within large datasets of academic papers, helping researchers uncover new trends and synthesize data with greater accuracy and efficiency. despite its potential, integrating ai into the literature review process raises several critical questions and challenges. for instance, the issue of trust and reliability remains a significant concern. ai models rely on vast amounts of training data, and any biases or inaccuracies in these datasets can affect the quality and objectivity of their reviews. furthermore, the interpretability of ai-generated insights is crucial; researchers must understand how and why the algorithms draw specific conclusions or summaries. in addition, there are concerns about the accessibility of ai tools, as their successful application often requires technical expertise and resources that may not be available to all researchers. finally, ethical considerations must be carefully addressed, such as ensuring fairness and transparency in automated processes. this literature review explores and evaluates the current state of ai in automating literature reviews. it examines the various ai technologies being employed, their advantages, and their limitations. by critically analyzing the progress and challenges in this field, this review aims to provide insights into the future potential of ai in academic research and offer guidance for researchers and practitioners looking to incorporate ai-driven methods into their literature review processes. the following sections will review both established ai tools and emerging experimental systems. finally, the paper will discuss the future directions of ai in literature review automation, considering how ongoing advancements in ai and machine learning could reshape academic research practices in the coming years. through this comprehensive review, we aim to provide a foundation for understanding the current capabilities of ai in this domain and offer perspectives on its evolving role in the academic landscape.	1	The method mentioned in this paper involves using a multi-LLM scoring mechanism to generate comprehensive survey outlines and content. This process begins with creating detailed outlines for various sections of the survey, which are then consolidated into a final, comprehensive outline. Each subsection is generated in parallel, guided by the outline, to accelerate the process. To address consistency issues, the method integrates a meta-evaluation step where human experts judge the quality of the generated surveys, ensuring alignment with human preferences.
2	Automated literature review using nlp techniques and llm based retrieval augmented generation		"Nurshat Fateh Ali Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka, Bangladesh nurshatfateh@gmail.com

Md. Mahdi Mohtasim Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka, Bangladesh mahdimohtasim@gmail.com

Shakil Mosharrof Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka, Bangladesh shakilmrf8@gmail.com

T. Gopi Krishna Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka, Bangladesh gopi.mistbd@gmail.com"	this research presents and compares multiple ap-proaches to automate the generation of literature reviews using several natural language processing (nlp) techniques and retrieval-augmented generation (rag) with a large language model (llm). the ever-increasing number of research articles provides a huge challenge for manual literature review. it has resulted in an increased demand for automation. developing a system capable of automatically generating the literature reviews from only the pdf files as input is the primary objective of this research work. the effectiveness of several natural language processing (nlp) strategies, such as the frequency-based method (spacy), the transformer model (simple t5), and retrieval-augmented generation (rag) with large language model (gpt-3.5-turbo), is evaluated to meet the primary objective. the scitldr dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews. the rouge scores are used for the evaluation of all three systems. based on the evaluation, the large language model gpt-3.5-turbo achieved the highest rouge-1 score, 0.364. the transformer model comes in second place and spacy is at the last position. finally, a graphical user interface is created for the best system based on the large language model. index terms —t5, spacy, large language model, gpt, rouge, literature review, natural language processing, retrieval-augmented generation.	literature reviews have gained considerable importance for scholars. it provides researchers with a comprehensive overview of previous findings in a specific field and assists scholars in identifying gaps in past understandings. it helps to conduct future research and informs researchers of areas where they can provide significant input. however, conducting liter-ature reviews can be incredibly cumbersome because there’s so much to read. due to the vast volume of research articles being released, reviewing all related studies and extracting automating aspects of the literature review process allows academicians to save time and concentrate on the most perti-nent articles for their research. it can also reduce the chance of errors or prejudice in the review process. the highlights of this article are: all three considered nlp approaches such as spacy, t5, and gpt-3.5-turbo-0125 model can produce satisfac-tory results in automating the literature review generation. • the llm-based model outperforms t5 and spacy in generating literature reviews. a framework was proposed by silva et al. [6] for auto-matically producing systematic literature reviews. they have focused on four technical steps: searching, screening, map-ping, and synthesizing. in response to a specific inquiry, extensive searches are conducted to find as much relevant research as feasible, involving looking through reference lists, scouring internet databases, and reviewing published materials. screening reduces the search scope by limiting the collection to only the papers pertinent to a particular review, aiming to highlight important findings and facts that could influence policy. mapping is used to comprehend research activity in a particular area, involve stakeholders, and define priorities concerning the review emphasis. synthesizing integrates data from numerous sources and provides an overview of the outcomes. the formulation of research questions, reporting phase, and peer review are some steps that are also discussed for the composition of systematic literature reviews. peer-reviewed publications are growing exponentially with the rapid development of science. therefore, yuan et al. [7] have explored the use of machine learning techniques, natural language generation, multi-document summarization, and multi-objective optimization for automating scientific re-viewing. they have discussed the generation of comprehensive reviews and noted the limitations of constructive feedback compared to human-written reviews. the models used in this research are not yet fully capable of automating literature reviews and they require human reviewers. a comprehensive analysis of existing tools for systematic literature reviews was done by karakan et al. [8]. they have explored the potential for automation in various phases of the review process, highlighting the need for a holistic tool de-sign to address researchers’ challenges effectively. they have discussed two methodologies to accomplish their research: rapid review and semi-structured interviews. rapid review emphasizes decision-making procedures for resolving issues, difficulties, and challenges that software engineers encounter in their daily work. semi-structured interviews are used to explore researchers’ experiences, challenges, strategies, strengths, weaknesses of systematic literature review tools, and requirements for effective support in software engineering. jaspers et al. [9] focused on the use of machine learning techniques for automation of literature reviews and systematic reviews. they have outlined the pros and cons of different machine-learning techniques. the process of automating the literature review was elaborately discussed. the paper lacks practical validation across diverse domains and detailed in-sights. a concise overview of automated literature reviews was presented by tauchert et. al. [10] they have emphasized the potential for automation in various stages of the systematic review process. the paper discusses the importance of in-tegrating computational techniques to streamline tasks such as searching, screening, extraction, and synthesis. it also ac-knowledges the need for further research to address challenges and enhance the effectiveness of automated approaches. a brief overview on the topic of automatic literature review tools was given by tsai et. al. [11] they discussed the existing research in the field, the challenges faced in conduct-ing literature reviews manually, and the potential benefits of automating the process. the main focus of their contributions is the evaluation of mistral llm’s effectiveness in the field of academic research. the gaps in the intersection of systematic literature reviews (slrs) and llms are discussed by susnjak et. al. [12]. they also emphasized the need to address challenges in the synthesis phase of research and highlighted the potential of fine-tuning llms with datasets to enhance knowledge synthesis accuracy. the study aims to bridge this gap by proposing a systematic literature review automation framework. most of the related works that have been discussed are mainly focused on discussing the potential and challenges of using nlp techniques and llms to automate the literature review process. none of them proposes a complete system pipeline where users can directly generate the literature re-view only using the pdf and doi. in contrast, this article proposes and implements three unique end-to-end pipelines and procedures for a literature review automation system. this research endeavor has also resulted in the implementation of a ui tool where users can directly upload pdfs and get a literature review segment generated automatically without any additional effort. moreover, this paper also includes a comparative analysis of different approaches such as the frequency-based approach, transformer-based approach, and rag-based approach using rouge scores which contributes towards finding the effectiveness of these approaches for this task.	0	The methods mentioned in the paper for automating literature reviews using NLP techniques and LLM-based retrieval include the frequency-based approach using spaCy, the transformer-based approach using the T5 model trained on the SciTLDR dataset, and the LLM-based approach using GPT-3.5-turbo. These methods were evaluated using ROUGE scores, with the LLM-based approach outperforming the others.
3	Automating research synthesis with domain specific large language model fine tuning		"Tianyu Gao Howard Yen Jiatong Yu Danqi Chen

Department of Computer Science & Princeton Language and Intelligence Princeton University {tianyug,hyen,jiatongy,danqic}@cs.princeton.edu"	large language models (llms) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. in this work, our aim is to allow llms to generate text with citations , improving their factual correctness and verifiability. existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. we propose alce , the first benchmark for a utomatic l lms’ c itation e valuation. alce collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. we develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. our experiments with state-of-the-art llms and novel prompting strategies show that current systems have considerable room for improvement—for example, on the eli5 dataset, even the best models lack complete citation support 50% of the time. our analyses further highlight promising future directions, including developing better retrievers, advancing long-context llms, and improving the ability to synthesize information from multiple sources.	large language models (llms; brown et al. , 2020 ; openai , 2023 ) have gained increasing popularity as a tool for information seeking. while they generate engaging and coherent responses, their outputs are prone to hallucination and often contain factually incorrect information ( ji et al. , 2023 ). this makes it harder for users to trust and verify llmgenerated outputs without any supporting evidence. in this work, we study a new generation paradigm for llms, in which we require llms to provide citations to one or a few text passages for any statement they generate (figure 1 ). incorporating citations brings several benefits: (1) users can easily verify llms’ claims with the provided citations; (2) llms can generate text that faithfully follows cited passages, which has the promise to improve correctness and alleviate hallucination. figure 1: the task setup of alce. given a question, the system generates text while providing citing passages from a large retrieval corpus. each statement may contain multiple citations (e.g., [1][2] ). multiple commercial systems have adopted this paradigm: bing chat 2 and perplexity.ai 3 respond to user questions in natural language with references to web pages. nakano et al. ( 2021 ); menick et al. ( 2022 ) share a similar motivation, but they mainly experiment with commercial search engines and closed-source models, making their results difficult to evaluate. retrieval-augmented lms ( borgeaud et al. , 2022 ; izacard et al. , 2022 ) incorporate retrieved passages during both training and inference, but do not guarantee faithfulness to retrieved passages or explicitly provide citations. additionally, previous studies mostly rely on human evaluation ( nakano et al. , 2021 ; menick et al. , 2022 ; liu et al. , 2023 ), which is expensive and difficult to reproduce. we argue that the absence of automated evaluation hinders the advances of such systems. table 1: the three datasets used in our alce benchmark. these datasets cover a wide range of question types and the corresponding corpora span from wikipedia to web-scale document collection. we present alce , the first reproducible benchmark for automatically evaluating llms’ generations with citations. alce assumes a naturallanguage question and a retrieval corpus, and requires building end-to-end systems to retrieve relevant passages from the corpus, generate a response to the question, and cite corresponding supporting passages. we compile three datasets that cover different types of questions and corpora— asqa ( stelmakh et al. , 2022 ), qampari ( rubin et al. , 2022 ), and eli5 ( fan et al. , 2019 )—as shown in table 1 . different from previous benchmarks ( lee et al. , 2019 ; bohnet et al. , 2022 ), alce evaluates long-text generation, focusing on automatically evaluating citation quality, and allows citing multiple passages for individual statements. we design automatic evaluation methods in three dimensions: fluency , correctness , and citation quality . specifically, we use mauve ( pillutla et al. , 2021 ) to measure fluency, propose tailored correctness metrics for each dataset, and adopt a natural language inference (nli) model ( honovich et al. , 2022 ) to measure citation quality. we showcase how the three dimensions together contribute to a robust evaluation, preventing systems from exploiting shortcuts. additionally, we conduct human evaluation and demonstrate a strong correlation with our automatic metrics. we experiment on multiple systems with stateof-the-art llms and retrievers and also propose novel prompting strategies to synthesize retrieved text into text generation. although all systems are capable of providing fluent and coherent responses, there remains substantial room for improvement in terms of correctness and citation quality: for example, on the eli5 dataset, around 50% generations of our chatgpt and gpt-4 baselines are not fully supported by the cited passages. additionally, we find that (1) a closed-book model (generating answers without accessing any retrieved documents) with post-hoc citing achieves good correctness but much worse citation quality; (2) although interactive retrieval approaches ( yao et al. , 2023 ; schick et al. , 2023 ) offer more flexibility in when/what to retrieve, they do not improve the performance on this challenging benchmark; (3) summarizing the retrieved passages in a shorter text improves correctness but not citation quality; (4) reranking multiple generations boosts citation quality measured by human evaluation; (5) incorporating more retrieved passages in context does not help chatgpt but improves gpt-4 performance. our extensive analyses highlight three major challenges of building llms to generate text with citations: (1) the retrieval quality is crucial to the final performance and has substantial room for improvement; (2) llms’ limited context window restricts the number of passages they can incorporate; (3) current llms struggle to synthesize multiple documents in context without being distracted by irrelevant ones, although better instruction tuning brings significant improvement. these challenges pose promising research directions for developing better systems integrating retrieval and llms.	2	The method mentioned in this paper discusses utilizing domain-specific large language models to automate the process of research synthesis. This involves training these models on specialized datasets to enhance their ability to accurately summarize, analyze, and integrate findings from multiple research studies within a specific field.
4	Automating systematic literature reviews with retrieval augmented generation a comprehensive overview		"Binglan Han , Teo Susnjak * and Anuradha Mathrani *

Citation: Han, B.; Susnjak, T.; Mathrani, A. Automating Systematic Literature Reviews with RetrievalAugmented Generation: A Comprehensive Overview. Appl. Sci. 2024 , 14 , 9103. https://doi.org/ 10.3390/app14199103

Academic Editor: Luis Javier Garcia Villalba

Received: 9 September 2024 Revised: 25 September 2024 Accepted: 27 September 2024 Published: 9 October 2024

School of Mathematical and Computational Sciences, Massey University, Auckland 0632, New Zealand; b.han1@massey.ac.nz * Correspondence: t.susnjak@massey.ac.nz (T.S.); a.s.mathrani@massey.ac.nz (A.M.)"	this study examines retrieval-augmented generation (rag) in large language models (llms) and their significant application for undertaking systematic literature reviews (slrs). rag-based llms can potentially automate tasks like data extraction, summarization, and trend identification. however, while llms are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. rag mitigates these limitations by integrating llms’ generative capabilities with the precision of real-time information retrieval. we review in detail the three key processes of the rag framework—retrieval, augmentation, and generation. we then discuss applications of rag-based llms to slr automation and highlight future research topics, including integration of domain-specific llms, multimodal data processing and generation, and utilization of multiple retrieval sources. we propose a framework of rag-based llms for automating srls, which covers four stages of slr process: literature search, literature screening, data extraction, and information synthesis. future research aims to optimize the interaction between llm selection, training strategies, rag techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends. keywords: retrieval-augmented generation; large language models; systematic literature review	rapidly developing large language models (llms) like gpt-4 [ 1 ] and llama [ 2 ] have transformed natural language processing (nlp) and artificial intelligence (ai) by generating human-like text and interpreting complex linguistic nuances across a wide range of fields. for example, llms have demonstrated rational thinking capabilities spanning drug discovery to personalized learning. however, such models are limited by the static knowledge they acquire during pre-training; this can lead to inaccuracies (especially in rapidly evolving fields). inaccuracies often include plausible sounding but factually incorrect responses or “hallucinations” [ 3 , 4 ]. moreover, these inaccuracies could be purposefully generated for malicious usage (e.g., promotion of certain drugs) by mixing authentic papers with fallacious papers to mislead the public [ 5 ]. retrieval-augmented generation (rag) addresses these limitations by combining the generative power of llms with the precision of real-time information retrieval. rag enhances llm performance by grounding responses in dynamically updated and retrievable content to improve accuracy and reliability. this approach is crucial in fields like law, medicine, finance, and personalized care, which critically require accurate, up-to-date information. additionally, rag mitigates hallucinations by anchoring outputs in verifiable sources, enabling users to trace and validate the information provided [ 6 , 7 ]. a systematic literature review (slr) methodically identifies, evaluates, and synthesizes existing research questions, systematically searching for relevant studies, screening and selecting pertinent on a specific topic. it typically involves several key processes: defining research questions, systematically searching for relevant studies, screening and selecting pertinent studies, extracting data, and analyzing and synthesizing subsequent findings. rag-based llms can facilitate and potentially automate these tasks to significantly improve efficiency and accuracy. this study provides a comprehensive overview of the primary methodologies associated with using rag in llms. furthermore, it examines the application of rag-based models to systematic literature reviews (slrs), identifying existing gaps and emerging trends in this domain. finally, this paper proposes a novel framework for automating systematic literature reviews through the application of rag-based llms. our proposed framework showcases a comprehensive solution for the automation of the four key stages of the slr process, namely, literature search, literature screening, data extraction, and information synthesis. it enhances both the efficiency and accuracy at each stage to provide this study therefore makes two signiﬁcant contributions with the automation of sys- this study therefore makes two significant contributions with the automation of systematic literature reviews: the in-depth review of rag-based llms and their applications to slrs establishes • the in-depth review of rag-based llms and their applications to slrs establishes a solid foundation for identifying existing research gaps and outlining future research  the proposed framework for rag-based llms comprehensively addresses the en- • the proposed framework for rag-based llms comprehensively addresses the entire slr process, accommodating its iterative and incremental nature. this framework provides a robust starting point for enhancing and automating slr tasks, thus contributing to the advancement of automation in this field.	0	The method mentioned in this paper for automating systematic literature reviews with retrieval-augmented generation (RAG) involves integrating RAG-based models to enhance the efficiency and accuracy of the SLR process. This includes automating key tasks such as systematically searching for relevant studies, screening and selecting pertinent studies, extracting data, and analyzing and synthesizing findings. By combining the generative capabilities of large language models (LLMs) with real-time information retrieval, RAG ensures that responses are grounded in dynamically updated and accurate content, thereby addressing the limitations of static knowledge in traditional LLMs.
5	Chatcite llm agent with human workflow guidance for comparative literature summary		"Yidong Wang 1 , ∗ , Qi Guo 2 , ∗ , Wenjin Yao 2 , Hongbo Zhang 1 , Xin Zhang 4 , Zhen Wu 3 , Meishan Zhang 4 , Xinyu Dai 3 , Min Zhang 4 , Qingsong Wen 5 , Wei Ye 2 † , Shikun Zhang 2 † , Yue Zhang 1 †

1 Westlake University, Peking University, 3 Nanjing University, Harbin Institute of Technology, Shenzhen, Squirrel AI"	this paper introduces autosurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods. while large language models (llms) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain. autosurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized llms, integration and refinement, and rigorous evaluation and iteration. our contributions include a comprehensive solution to the survey problem, a reliable evaluation method, and experimental validation demonstrating autosurvey’s effectiveness. we open our resources at https://github.com/autosurveys/autosurvey .	"survey papers provide essential academic resources, offering comprehensive overviews of recent research developments, highlighting ongoing trends, and identifying future directions [ 1 – 4 ]. however, crafting these surveys is increasingly challenging, especially in the fast-paced domain of artificial intelligence including large language models(llms) [ 5 – 8 ]. figure 1a illustrates a significant trend: in just the first four months of 2024 alone, over 4,000 papers containing the phrase ""large language model"" in their titles or abstracts were submitted to arxiv. this surge highlights a critical academic issue: the rapid accumulation of new information often outpaces the capacity for comprehensive scholarly review and synthesis, emphasizing the growing need for more efficient methods to synthesize the expanding literature. moreover, as depicted in figure 1b, while the number of survey papers has rapidly increased, the growing difficulty of producing traditional human-authored survey papers—due to the sheer volume and complexity of data—remains a significant challenge. this challenge is evidenced by the lack of comprehensive surveys in many fields (figure 1c), which hinders knowledge transfer and makes it difficult for new researchers to efficiently navigate the vast amount of available information. the advent of llms [ 7 , 9 ] presents a promising avenue for addressing these challenges. these models, trained on extensive text corpora, demonstrate remarkable capabilities in understanding and generating human-like text, even in long-context scenarios [ 10 – 12 ]. despite these advancements, the practical application of llms to survey generation is fraught with challenges. firstly, context window limitations : llms encounter inherent restrictions in output length due to limited processing windows [ 13 – 17 ]. while several advanced large models, including gpt-4 and claude 3, support inputs exceeding 100k tokens, their output is still limited to fewer than 8k tokens (the output length of gpt-4 is 8k, and the output length of claude 3 is 4k). writing a comprehensive survey typically requires reading hundreds of papers, resulting in input sizes far beyond the capacity of even the most advanced models. moreover, a well-written survey itself spans tens of thousands of tokens, making it highly challenging to generate such extensive content directly with large models. secondly, parametric knowledge constraints : sole reliance on an llm’s internal knowledge is insufficient for producing surveys that require comprehensive and accurate references [ 18 – 20 ]. llms may generate content based on inaccuracies or even non-existent “hallucinated” references. moreover, these models cannot incorporate the latest studies not included in their training data, which limits the breadth and depth of the surveys they generate. thirdly, the lack of evaluation benchmark: after production, reliable metrics to evaluate the quality of outputs from llms are lacking. relying on human review for quality assessment is not only resource-intensive but also lacks scalability [ 21 – 23 ]. this presents a significant obstacle to the widespread adoption of llms for academic synthesis, where rigorous standards of accuracy and reliability are paramount. figure 1: depicting growth trends from 2019 to 2024 in the number of llms-related papers (a) and surveys (b) on arxiv, accompanied by a t-sne visualization. the data for 2024 is up to april, with a red bar representing the forecasted numbers for the entire year. while the number of surveys is increasing rapidly, the visualization reveals areas where comprehensive surveys are still lacking, despite the overall growth in survey numbers. the research topics of the clusters in the t-sne plot are generated using gpt-4 to describe their primary focus areas. these clusters of research voids can be addressed using autosurvey at a cost of \$1.2 (cost analysis in appendix d) and 3 minutes per survey. an example survey focused on emotion recognition using llms is in appendix f. in response to these challenges, we introduce autosurvey: a speedy and well-organized methodology for conducting comprehensive literature surveys. specifically, autosurvey’s primary innovations include: logical parallel generation : autosurvey employs a two-stage generation approach to parallelly generate survey content efficiently. initially, multiple llms work concurrently to create detailed outlines. a final, comprehensive outline is then synthesized from these individual outlines, setting a clear framework for content development. subsequently, each subsection of the survey is generated in parallel and guided by the outline, which significantly accelerates the process. to overcome potential transition and consistency issues due to segmented generation phases, autosurvey integrates a systematic revision phase. after the initial parallel generation, each section undergoes thorough revision and polishing, ensuring smooth transitions and enhanced overall document consistency. the sections are then seamlessly merged to produce a cohesive and well-organized final document. real-time knowledge update : autosurvey incorporates a real-time knowledge update mechanism using a retrieval-augmented generation (rag) approach [ 24 – 26 ]. this feature ensures that every aspect of the survey reflects the most current studies. when a survey topic is input by the user, autosurvey leverages the rag system to retrieve the latest relevant papers, forming the basis for generating a structured and informed outline. during subsection writing, the system dynamically pulls in new research articles relevant to the specific content under development. this approach ensures that citations are current and the survey content is aligned with the latest developments in the field, significantly enhancing the accuracy and depth of the literature review. muti-llm-as-judge evaluation : autosurvey employs the multi-llm-as-judge strategy, leveraging the llm-as-judge method for text evaluation [ 22 , 21 , 23 ]. this approach generates initial evaluation metrics using multiple large language models, which process a substantial corpus of high-quality surveys. these metrics are refined by human experts to ensure precision and adherence to academic standards. the multi-llm-as-judge method assesses generated content across two main dimensions: (1) citation quality, verifying the accuracy and reliability of the information presented, with sub-indicators for recall and precision. (2) content quality, consisting of coverage (assessing the extent of topic encapsulation), structure (evaluating logical organization and coherence), and relevance (ensuring alignment with the main topic). by utilizing multiple llms, this strategy minimizes bias and ensures a balanced and comprehensive assessment, upholding rigorous academic standards. figure 2: the autosurvey pipeline for generating comprehensive surveys. extensive experimental results across different survey lengths (8k, 16k, 32k, and 64k tokens) demonstrate that autosurvey consistently achieves high citation and content quality scores. at 64k tokens, autosurvey achieves 82.25% recall and 77.41% precision in citation quality, outperforming naive rag-based llms (68.79% recall and 61.97% precision) and approaching human performance (86.33% recall and 77.78% precision). in content quality at 64k tokens, autosurvey scores 4.73 in coverage, 4.33 in structure, and 4.86 in relevance, closely aligning with human performance (5.00, 4.66, and 5.00 respectively). at shorter lengths (8k, 16k, and 32k tokens), autosurvey also maintains strong performance across all metrics. furthermore, the spearman’s rho values indicate a moderate positive correlation between the rankings provided by the llms and those given by human experts. the mixture of models achieves the highest correlation at 0.5429, indicating a strong alignment with human preferences. these results reinforce the effectiveness of our multi-llm scoring mechanism, providing a reliable proxy for human judgment across varying survey lengths. in conclusion, to the best of our knowledge, autosurvey is the first system to explore the potential of large model agents in writing extensive academic surveys. it proposes evaluation criteria for surveys that align with human preferences, providing a valuable reference for future related research."	1	The method mentioned in this paper discusses an agent designed with human workflow guidance to generate literature summaries, enhancing the stability and quality of the summaries produced. This approach differs from the simple Chain-of-Thought (CoT) prompting method by integrating a structured human-like workflow, which guides the generation process more effectively, leading to higher-quality and more coherent summaries.
6	Cluster based effective generation of ai driven literature surveys		"Yutong Li 1 , Lu Chen 2 , , Aiwei Liu 1 , Kai Yu 2 , ,Lijie Wen 1

1 Tsinghua University, Beijing, China 2 X-LANCE Lab, Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence, SJTU AI Institute Shanghai Jiao Tong University, Shanghai, China 3 Suzhou Laboratory, Suzhou, China

li-yt21@mails.tsinghua.edu.cn , chenlusz@sjtu.edu.cn , wenlj@tsinghua.edu.cn"	the literature review is an indispensable step in the research process. it provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. however, literature summary is challenging and time consuming. the previous llm-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. however, for the summarization step, simple cot method often lacks the ability to provide extensive comparative summary. in this work, we firstly focus on the independent literature summarization step and introduce chatcite 1 , an llm agent with human workflow guidance for comparative literature summary. this agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a reflective incremental mechanism. in order to better evaluate the quality of the generated summaries, we devised a llm-based automatic evaluation metric, g-score, in refer to the human evaluation criteria. the chatcite agent outperformed other models in various dimensions in the experiments. the literature summaries generated by chatcite can also be directly used for drafting literature reviews.	as the rapid advancement of academic research, scholars must delve into existing literature to understand past studies, recognize future research trends, and find innovative approaches in their fields. crafting a literature review entails searching for relevant literature and conducting detailed comparative summarization. it typically involves two main steps: literature collection followed by literature summary generation based on the collected sources. however, organizing a high-quality literature review necessitates scholars to engage in thorough analysis, organization, comparison, and integration of an extensive of related works, which is often a challenging and time-consuming task. figure 1: literature summary task description therefore, hoang and kan ( 2010 ) have proposed the automatic generation of literature summary. however, machine-generated literature summaries often encounter challenges like information omission, lack of linguistic fluency, and insufficient comparative analysis. in traditional models, summaries generated through extraction and abstraction approach may miss key information due to the limitations of the model, leading to the lack of crucial points or findings of the generated summaries. some automated systems may lack the ability for in-depth comparative analysis, potentially resulting in literature summaries that lack a comprehensive understanding of the relevant research in the field. in recent years, with the rapid development of large language models (llms) ( radford et al. , 2019 ; brown et al. , 2020 ), their powerful capabilities in natural language generation tasks have been demonstrated across various tasks, that provides possibilities for handling longer texts and generating comprehensive summaries. researchers have started exploring how to leverage llms to generate automatic literature summaries. wei et al. ( 2023 ) propose a chain-of-thought (cot) prompting method to enhance the ability of large language models to perform complex reasoning. cot allows llms to devise their own plan, resulting in generated text that aligns more closely with human preferences.recent study by ( huang and tan , 2023 ) and agarwal et al. ( 2024 ) on literature review has focused more on how to retrieve relevant papers more accurately and neglected research on literature summarization. they use only simple cot guidance to generate literature summaries, resulting in a lack of comparative and organizational analysis. large language models, despite their fluent language generation, struggle to consistently produce comparative literature summaries due to their unpredictable an stochastic nature. the length limitations of these models require a two-step summarization approach, increasing the risk of information omission during abstract generation. in this work, we focus on the independent literature summarization task, aiming to generate a comprehensive comparative literature summary through a certain collection of literature and a description of the proposed work, as illustrated in figure 1 . to address these challenges mentioned above, our work proposes chatcite , a llm-based agent guided by human workflow. different from simple cot prompting approach, the agent is designed with the human workflow guidance, rather than formulating the generation process in a blackbox manner, ensuring a more stable generation of higher-quality generic summaries. furthermore, quality assessment for generative tasks has always been a challenge. prior studies on literature summarization have primarily relied on text summarization metrics, such as rouge ( lin ( 2004a )). however, traditional text summary evaluation metrics, like rouge, are not sufficient to assess the quality of literature summaries. more comprehensive evaluation criteria covering multiple dimensions are required to ensure that the generated literature summaries truly meet the requirements. therefore, we combine human studies on literature reviews ( justitia and wang , 2022 ) to formulate the evaluation criteria for literature summaries from multiple dimensions 2 , and propose an llm-based automatic evaluation metric, g-score. experimental results demonstrate its consistency with human evaluations. in this paper, we summarize our main contributions of our framework as follows: we focus on the independent literature summarization step of literature review, and introduce chatcite , an llm agent with human workflow guidance for comparative literature summary. based on research on literature summaries, we have developed a multidimensional quality assessment criterion for literature summaries. additionally, we propose an llm-based automatic evaluation metric, g-score, demonstrat- ing results consistent with human preferences. • the experimental results indicate that chatcite outperforms other llm-based literature summarization methods in all quality dimensions. the literature summaries produced by chatcite can be directly utilized for drafting literature reviews. • we demonstrate that llms with human workflow guidance, have the ability to effectively perform comprehensive comparative summarization of multiple documents. therefore, we infer that large language models (llms) have the potential to handle more complex inferential summarization tasks.	1	The method mentioned in this paper discusses the use of a clustering approach combined with Retrieval-Augmented Generation (RAG) and large language models (LLMs) to effectively generate AI-driven literature surveys. This method involves structuring the generation process through carefully designed prompts and utilizing an LLM API with specific configurations to balance creativity and precision. The clustering method helps in managing large sets of references and generating structured, coherent outlines, which are then integrated into a comprehensive literature survey. The effectiveness of this approach is demonstrated through participation in the NLPCC 2024 shared task 6, achieving superior scores in metrics such as soft heading recall and ROUGE.
7	Enabling large language models to generate text with citations		"A P REPRINT

Teo Susnjak ∗ School of Mathematical and Computational Sciences Massey University Albany, New Zealand

Peter Hwang School of Mathematical and Computational Sciences Massey University Albany, New Zealand

Napoleon H. Reyes School of Mathematical and Computational Sciences Massey University Albany, New Zealand

Andre L. C. Barczak Centre for Data Analytics Bond University Gold Coast, Australia Timothy R. McIntosh Cyberoo Pty Ltd Surry Hills, NSW Australia

Surangika Ranathunga School of Mathematical and Computational Sciences Massey University Albany, New Zealand

April 16, 2024"	this research pioneers the use of finetuned large language models (llms) to automate systematic literature reviews (slrs), presenting a significant and novel contribution in integrating ai to enhance academic research methodologies. our study employed the latest finetuning methodologies together with open-sourced llms, and demonstrated a practical and efficient approach to automating the final execution stages of an slr process that involves knowledge synthesis. the results maintained high fidelity in factual accuracy in llm responses, and were validated through the replication of an existing prisma-conforming slr. our research proposed solutions for mitigating llm hallucination and proposed mechanisms for tracking llm responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. the findings ultimately confirmed the potential of finetuned llms in streamlining various labour-intensive processes of conducting literature reviews. given the potential of this approach and its applicability across all research domains, this foundational study also advocated for updating prisma reporting guidelines to incorporate ai-driven processes, ensuring methodological transparency and reliability in future slrs. this study broadens the appeal of ai-enhanced tools across various academic and research fields, setting a new standard for conducting comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies.	systematic literature reviews (slrs) serve as the bedrock of academic research, playing a crucial role in the amalgamation, examination, and synthesis of existing scholarly knowledge across various fields[ 1 , 2 , 3 ]. these reviews offer a methodical and replicable approach, ensuring the integrity and thoroughness of research synthesis especially when combined with reporting guidelines like prisma [ 4 , 5 , 6 ]. such a foundation is indispensable for advancing both theoretical understanding and practical applications. however, the traditional execution of slrs is marked by its manual and resource-intensive nature, often stretching over extensive periods, which introduces significant inefficiencies into the research process [7, 8]. the rigorous yet cumbersome character of traditional slr methodologies presents considerable bottlenecks in the management and synthesis of large datasets of selected studies that hinges on effective information retrieval [ 9 , 10 ]. these challenges not only prolong the synthesis - the execution phase of a review - but also hamper the ongoing updates of the slrs with newer findings, and risk diminishing the timeliness and relevance of the insights gleaned[ 11 , 12 ]. this scenario underscores the need for innovative, scalable and sustainable solutions that can streamline the extraction of information from findings situated in academic papers, as well as its persistence in suitable information technologies which facilitate their accurate and effective retrieval necessary for executing slrs [13, 14]. the recent advent of a new class of artificial intelligence (ai) systems like large language models (llms), heralds a new epoch with the potential to dramatically redefine the slr landscape through the automation of the information retrieval processes while maintaining high factual fidelity [ 15 , 8 ]. these models, with their advanced natural language comprehension capabilities, text generation and knowledge retention [ 16 ], offer a promising avenue for automating and optimizing various stages of the slr process [ 17 ], and in particular the execution phase that relies on “talking to” both individual academic papers via llms, as well as simultaneously “talking across” all target papers for synthesising purposes [ 18 ]. despite their potential, the broad generalist pretraining of these models which have been trained on vast amounts of diverse text data means that the llms fall short in providing the domain-specific accuracy and precision in the information retrieved that is essential for the detailed task of knowledge synthesis across a very specific and narrow sets of studies. additionally, their current propensity to hallucinate [ 19 , 20 , 21 , 22 ] renders them unable to consistently respond accurately, which precludes them from providing reliable responses needed to conduct slrs with integrity. additionally, current llms have variable abilities to audit and track the sources of their responses [ 23 ]. their inability to reliably ensure that the provenance of the llm responses can be linked to the target studies that comprise an slr corpus represents a serious limitation for using this technology for this purpose. collectively, these gaps highlight a critical area within ai-assisted slr processes for the purpose of enhancing the information retrieval capabilities of llms [ 24 ]. indeed, resent research has repeatedly raised these concerns with respect to using llm for the purposes of slrs, especially in the knowledge synthesis stages. while it has been suggested that llms could be used for assisting evidence synthesis tasks in slrs via their summarization capabilities, concerns have been raised about their lack of continuous learning capability and temporal reasoning [ 25 , 26 ]. qureshi et al. [27] noted that using llms shows promise for aiding in systematic review-related tasks, but the authors concluded that the technology is in its infancy and requires significant development for such applications. in their most recent review, bolanos et al. [28] found that llm usage for slrs is hampered by limitations such as reduced efficacy in domain-specific and narrower subject areas, their propensity for hallucinating and generating misleading information, alongside their opaque decision processes which cannot be audited. to surmount these challenges, this study proposes the creation of finetuned open-source llms, trained on the corpus of selected academic papers for a target slr, expanding the generalist knowledge of an llm with narrower domainspecific expertise. this work devises a novel way to automatically extract information from a set of academic papers in order to create slr-specific datasets which can be leveraged for finetuning llms so that they can support question and answering downstream tasks. we also devise mechanisms to mitigate llm hallucination and to ensure that all llm responses related to an slr can be tracked to source studies. the ensuing research presents a comprehensive slr-automation framework with a focus on the knowledge synthesis stage that aims to revolutionize information retrieval mechanisms with empirical evidence of their effectiveness, thereby expediting and transforming the synthesis of research findings in the context of slrs. contribution our study substantially contributes to the field of information retrieval in general where factual recall from arrays of documents is required and needs to be expressed in natural language, while possessing the ability to audit the responses with respect to the source information. our proposed framework is specifically applied and demonstrated to the context of slr-automation, where the goal was to validate the proposed framework through an empirical study that seeks to replicate a previously published prisma-conforming slr, which serves as a gold standard and a test use case for validating the framework. the contributions of this research can be summarised as: devising a methodical and automate approach for converting selected academic papers into datasets that can be used for finetuning llms. • proposing an effective approach to ensuring factual recall in responses by providing a mechanism to audit the source information of the llm’s response. • developed and validated evaluation metrics with the goal of testing for factuality in the llm responses. • benchmarked various ai automation methodologies for slrs, focusing on the comparative effectiveness of different finetuning approaches. • demonstrated the efficacy of these proposed methodologies by replicating a published prisma-conforming slr. • enhancing the scholarly toolkit for slrs with advanced, efficient, and context-aware ai technologies. • setting new standards in academic research for reliability, validity, and ethical ai employment through pioneering ai methodologies. • releasing a python package 2 to facilitate data curation for llm fine-tuning, tailored for the unique requirements of slrs.	2	The method mentioned in this paper involves requiring large language models (LLMs) to provide citations to one or a few text passages for any statement they generate. This approach, referred to as ALCE (Automatic Long-Context Evaluation), ensures that LLMs generate text that is verifiable and faithful to the cited passages, thereby improving correctness and reducing hallucination. The system retrieves relevant passages from a large corpus, and the LLMs are prompted to cite these passages when generating responses to questions.
