{
    "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
    "authors": "Yutong Li 1 , Lu Chen 2 , , Aiwei Liu 1 , Kai Yu 2 , ,Lijie Wen 1\n\n1 Tsinghua University, Beijing, China 2 X-LANCE Lab, Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence, SJTU AI Institute Shanghai Jiao Tong University, Shanghai, China 3 Suzhou Laboratory, Suzhou, China\n\nli-yt21@mails.tsinghua.edu.cn , chenlusz@sjtu.edu.cn , wenlj@tsinghua.edu.cn",
    "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite 1 , an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.",
    "introduction": "As the rapid advancement of academic research, scholars must delve into existing literature to understand past studies, recognize future research trends, and find innovative approaches in their fields. Crafting a literature review entails searching for relevant literature and conducting detailed comparative summarization. It typically involves two main steps: literature collection followed by literature summary generation based on the collected sources. However, organizing a high-quality literature review necessitates scholars to engage in thorough analysis, organization, comparison, and integration of an extensive of related works, which is often a challenging and time-consuming task.\n\nFigure 1: Literature Summary Task Description\n\nTherefore, Hoang and Kan ( 2010 ) have proposed the automatic generation of literature summary. However, machine-generated literature summaries often encounter challenges like information omission, lack of linguistic fluency, and insufficient comparative analysis. In traditional models, summaries generated through extraction and abstraction approach may miss key information due to the limitations of the model, leading to the lack of crucial points or findings of the generated summaries. Some automated systems may lack the ability for in-depth comparative analysis, potentially resulting in literature summaries that lack a comprehensive understanding of the relevant research in the field.\n\nIn recent years, with the rapid development of large language models (LLMs) ( Radford et al. , 2019 ; Brown et al. , 2020 ), their powerful capabilities in natural language generation tasks have been demonstrated across various tasks, that provides possibilities for handling longer texts and generating comprehensive summaries. Researchers have started exploring how to leverage LLMs to generate automatic literature summaries. Wei et al. ( 2023 ) propose a Chain-of-Thought (CoT) prompting method to enhance the ability of large language models to perform complex reasoning. CoT allows LLMs to devise their own plan, resulting in generated text that aligns more closely with human preferences.Recent study by ( Huang and Tan , 2023 ) and Agarwal et al. ( 2024 ) on literature review has focused more on how to retrieve relevant papers more accurately and neglected research on literature summarization. They use only simple CoT guidance to generate literature summaries, resulting in a lack of comparative and organizational analysis. Large language models, despite their fluent language generation, struggle to consistently produce comparative literature summaries due to their unpredictable an stochastic nature. The length limitations of these models require a two-step summarization approach, increasing the risk of information omission during abstract generation.\n\nIn this work, we focus on the independent literature summarization task, aiming to generate a comprehensive comparative literature summary through a certain collection of literature and a description of the proposed work, as illustrated in Figure 1 . To address these challenges mentioned above, our work proposes ChatCite , a LLM-based agent guided by human workflow. Different from simple CoT prompting approach, the agent is designed with the human workflow guidance, rather than formulating the generation process in a blackbox manner, ensuring a more stable generation of higher-quality generic summaries.\n\nFurthermore, quality assessment for generative tasks has always been a challenge. Prior studies on literature summarization have primarily relied on text summarization metrics, such as ROUGE ( Lin ( 2004a )). However, traditional text summary evaluation metrics, like ROUGE, are not sufficient to assess the quality of literature summaries. More comprehensive evaluation criteria covering multiple dimensions are required to ensure that the generated literature summaries truly meet the requirements. Therefore, we combine human studies on literature reviews ( Justitia and Wang , 2022 ) to formulate the evaluation criteria for literature summaries from multiple dimensions 2 , and propose an LLM-based automatic evaluation metric, G-Score. Experimental results demonstrate its consistency with human evaluations.\n\nIn this paper, we summarize our main contributions of our framework as follows:\n\nwe focus on the independent literature summarization step of literature review, and introduce ChatCite , an LLM agent with human workflow guidance for comparative literature summary.\n\nBased on research on literature summaries, we have developed a multidimensional quality assessment criterion for literature summaries. Additionally, we propose an LLM-based automatic evaluation metric, G-Score, demonstrat- ing results consistent with human preferences. • The experimental results indicate that ChatCite outperforms other LLM-based literature summarization methods in all quality dimensions. The literature summaries produced by ChatCite can be directly utilized for drafting literature reviews. • We demonstrate that LLMs with human workflow guidance, have the ability to effectively perform comprehensive comparative summarization of multiple documents. Therefore, we infer that Large Language Models (LLMs) have the potential to handle more complex inferential summarization tasks.",
    "main_content": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary\n\nYutong Li 1 , Lu Chen 2 , , Aiwei Liu 1 , Kai Yu 2 , ,Lijie Wen 1\n\n1 Tsinghua University, Beijing, China 2 X-LANCE Lab, Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence, SJTU AI Institute Shanghai Jiao Tong University, Shanghai, China 3 Suzhou Laboratory, Suzhou, China\n\nli-yt21@mails.tsinghua.edu.cn , chenlusz@sjtu.edu.cn , wenlj@tsinghua.edu.cn\n\nAbstract\n\nThe literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite 1 , an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.\n\n1 Introduction\n\nAs the rapid advancement of academic research, scholars must delve into existing literature to understand past studies, recognize future research trends, and find innovative approaches in their fields. Crafting a literature review entails searching for relevant literature and conducting detailed comparative summarization. It typically involves two main steps: literature collection followed by literature summary generation based on the collected sources. However, organizing a high-quality literature review necessitates scholars to engage in thorough analysis, organization, comparison, and integration of an extensive of related works, which is often a challenging and time-consuming task.\n\nFigure 1: Literature Summary Task Description\n\nTherefore, Hoang and Kan ( 2010 ) have proposed the automatic generation of literature summary. However, machine-generated literature summaries often encounter challenges like information omission, lack of linguistic fluency, and insufficient comparative analysis. In traditional models, summaries generated through extraction and abstraction approach may miss key information due to the limitations of the model, leading to the lack of crucial points or findings of the generated summaries. Some automated systems may lack the ability for in-depth comparative analysis, potentially resulting in literature summaries that lack a comprehensive understanding of the relevant research in the field.\n\nIn recent years, with the rapid development of large language models (LLMs) ( Radford et al. , 2019 ; Brown et al. , 2020 ), their powerful capabilities in natural language generation tasks have been demonstrated across various tasks, that provides possibilities for handling longer texts and generating comprehensive summaries. Researchers have started exploring how to leverage LLMs to generate automatic literature summaries. Wei et al. ( 2023 ) propose a Chain-of-Thought (CoT) prompting method to enhance the ability of large language models to perform complex reasoning. CoT allows LLMs to devise their own plan, resulting in generated text that aligns more closely with human preferences.Recent study by ( Huang and Tan , 2023 ) and Agarwal et al. ( 2024 ) on literature review has focused more on how to retrieve relevant papers more accurately and neglected research on literature summarization. They use only simple CoT guidance to generate literature summaries, resulting in a lack of comparative and organizational analysis. Large language models, despite their fluent language generation, struggle to consistently produce comparative literature summaries due to their unpredictable an stochastic nature. The length limitations of these models require a two-step summarization approach, increasing the risk of information omission during abstract generation.\n\nIn this work, we focus on the independent literature summarization task, aiming to generate a comprehensive comparative literature summary through a certain collection of literature and a description of the proposed work, as illustrated in Figure 1 . To address these challenges mentioned above, our work proposes ChatCite , a LLM-based agent guided by human workflow. Different from simple CoT prompting approach, the agent is designed with the human workflow guidance, rather than formulating the generation process in a blackbox manner, ensuring a more stable generation of higher-quality generic summaries.\n\nFurthermore, quality assessment for generative tasks has always been a challenge. Prior studies on literature summarization have primarily relied on text summarization metrics, such as ROUGE ( Lin ( 2004a )). However, traditional text summary evaluation metrics, like ROUGE, are not sufficient to assess the quality of literature summaries. More comprehensive evaluation criteria covering multiple dimensions are required to ensure that the generated literature summaries truly meet the requirements. Therefore, we combine human studies on literature reviews ( Justitia and Wang , 2022 ) to formulate the evaluation criteria for literature summaries from multiple dimensions 2 , and propose an LLM-based automatic evaluation metric, G-Score. Experimental results demonstrate its consistency with human evaluations.\n\nIn this paper, we summarize our main contributions of our framework as follows:\n\nwe focus on the independent literature summarization step of literature review, and introduce ChatCite , an LLM agent with human workflow guidance for comparative literature summary.\n\nBased on research on literature summaries, we have developed a multidimensional quality assessment criterion for literature summaries. Additionally, we propose an LLM-based automatic evaluation metric, G-Score, demonstrat- ing results consistent with human preferences. • The experimental results indicate that ChatCite outperforms other LLM-based literature summarization methods in all quality dimensions. The literature summaries produced by ChatCite can be directly utilized for drafting literature reviews. • We demonstrate that LLMs with human workflow guidance, have the ability to effectively perform comprehensive comparative summarization of multiple documents. Therefore, we infer that Large Language Models (LLMs) have the potential to handle more complex inferential summarization tasks.\n\n2 Related Work 3\n\nIn recent years, there is abundant research on generated literature summaries with the initial proposal made by Hoang and Kan ( 2010 ), to automate related work summarization created by a topicrelated work summary based on an extractive approach. To generate citation sentence, Xing et al. ( 2020 ) adopted a multi-source pointer-generator network with cross-attention mechanism, while AbuRa’ed et al. ( 2020 ) utilized the ARWG sys- tem, employing a neural sequence learning process and Ge et al. ( 2021 ) proposed a BACO framework based on background knowledge and content. Furthermore, Chen et al. ( 2021 ) employed the Relationaware Related Work Generator (RRG) to generate citation paragraphs while Chen et al. ( 2022 ) applied contrastive learning to generate target-aware related work segments. Yet traditional generation methods cannot generate the conprehensive coherent literature review due to the size of their model and the lack of the coherent and procedural language continuity.\n\nLarge Language Models (LLMs), such as GPT ( Radford et al. ( 2019 ), Brown et al. ( 2020 )), have demonstrated their powerful capabilities in natural language generation tasks. The study by Huang and Tan ( 2023 ) on the use of AI tools like ChatGPT in writing scientific review articles reveals the potential benefits and drawbacks of artificial intelligence in academic writing. Building on these insights, Agarwal et al. ( 2024 ) introduces the LitLLM toolkit, which overcomes challenges such as generating hallucinated content and overlooking recent research by adopting Retrieval Augmented Generation (RAG) principles, specialized prompting, and instructive techniques. However, these studies only applied a simple Chain of Thought (CoT) to the search and filtering process in literature reviews, resulting in poor readability. By comparison, ChatCite focuses on the independent task of text summarization, aiming to generate higher-quality summaries.\n\nFurthermore, this paper introduced a multidimensional G-Score evaluation metric inspired by the previous attempt to use Large Language Models (LLMs) through chain-of-thought methods to evaluate the quality of natural language generation (NLG) systems ( Liu et al. ( 2023 ), Goyal et al. ( 2023 )) which is more consistent with human evaluation compared to traditional ROUGE metrics ( Lin ( 2004b )).\n\n3 ChatCite\n\nThe literature review task can be decomposed into two sub tasks: relevant papers retrieval and literature summaries generation. This work focuses on the independent task of literature summary generation. Our task is to generate the literature summary based on the proposed work description D and a certain reference papers set R = { r 1 , r 2 , ..., r n } . Given D and R , our agent generates a literature summary Y = f ( D , R ) .\n\nDiverging from other types of summaries, such as news summaries, the literature summary generated directly by large language models using simple Chain-of-Thought (CoT) guidance in existing work mainly faces the following issues:\n\nKey Elements missing: Because of the window limitations of LLMs, generating the complete literature review directly is challenging. Typically, a two-step approach is used involving summarization and literature review generation. However, this process can lead to the loss of key elements during summarization. Even if the entire literature summary can be directly generated, using the entire text may result in mistakes in understanding key elements and the loss of such elements.\n\nLack of Comparative Analysis: Comparative analysis is crucial in literature summary, requiring an analysis on the limitations and advantages of existing research methods, and focusing on differences and similarities in methods, experimental design, dataset usage, and more. Directly using CoT-generated results often lacks comparative analysis.\n\nLack of Organizational Structure: The literature summary generated solely by CoT tends to be discrete for each paper, lacking classification for similar works and an organized structure for the literature review.\n\nTo address these challenges, we have proposed an LLM agent for comparative literature summary with human workflow guidance, ChatCite , consisting two modules: the Key Element Extractor and the Reflective Incremental Generator, as illustrated in Figure 2 . In this process, we utilize large language models as both generation and evaluation components, eliminating the need for additional model training and improving the quality of generated text to some extent.\n\nThe generation process guided by human workflow is as follows:\n\nThe proposed work description and reference papers in the reference papers set are initially processed using the Key Element Extractor separately.\n\nIteratively generate literature summaries using reference papers set. In each iteration, use the comparative summarizer to generate a comparative analysis summary. Then, use the reflective evaluator to vote on the generated candidate results, ranking the vote score and retaining the top n c results. Iterate continuously until all reference papers are processed.\n\nThe final output is selected based on the highest voting score among the generated related work summaries.\n\nIn this section, we first elaborate on the specifics of the Key Element Extractor (§ 3.1 ) and the Reflective Iterative Generator module (§ 3.2 ) in detail.\n\nFigure 2: The ChatCite consists of two modules, the Key Element Extractor and the Reflective Incremental Generator. The agent mimicking human workflow generates literature summary utilizing the Key Element Extractor to process the proposed work description and reference paper in Reference Papers Set. It then iteratively generates literature summaries using each paper in the Reference Papers Set, proposed work key elements and previous summary generated with the Reflective Incremental Generator. This process is iteratively repeated until a complete related work summary is generated, and the optimal one is selected as the final result.\n\n3.1Key Element Extractor\n\nIn order to retain sufficient key element for literature summary, we create seven simple guiding questions based on analysis ( Justitia and Wang , 2022 ) on literature review. We concatenate theses questions and the content required extraction as prompt to instruct LLMs extract the key elements. For each element, a simple question (shown in Figure 2 ) is set to guide the model in extraction, and these questions are Q e = [ q 1 , q 2 , ..., q 7 ] . These questions Q e and paper content C are concatenated to form the key element extraction prompt P e = [ Q e , C ] . Using LLM as extraction decoder to extract key elements and storing them in memory.\n\n3.2 Reflective incremental Generator\n\nTo overcome the challenges of lacking comparative analysis and organizational structure in literature reviews generated by LLMs, we designed the reflective incremental generator. The generator uses the Comparative Summarizer to continue writing comparative summaries, combining the results from the previous turn and the key elements of the proposed work and reference papers. It then utilizes the reflective evaluator to filter the generated results. This process is interatively applied to each reference paper in the reference papers set until all reference papers are processed. The best result is ultimately retained as the model’s generated output.\n\n3.2.1 Comparative Summarizer\n\nFor turn i , based on the proposed work key element pro , the key element of the i -th reference paper ref i and comparative summarization guidance sequentially generated summary for each summary s ∈ S i − 1 , and generating n s samples each time. Si = {G(Dg, pro, refi, s, ns), ∀s ∈Si−1} Here,to enhance the comparability and organization of the generated summaries, comparative summarization guidance are provided: \"Considering the rela- tionship between the reference paper and the target paper, as well as existing references in the previously completed related work, while retaining the content of all referenced papers mentioned in the previously completed related work.\"\n\n3.2.2 Reflective Mechanism\n\nDue to significant uncertainty in text generation tasks, we employ reflective generation to enhance the quality and stability of generated paragraphs. Here, we use LLMs as Reflective Evaluator to vote n v times on the generated results in each turn and then perform a statistical analysis on the voting results to obtain voting scores E i = E ( D e , S i ′ ) .\n\nThen we sort the scores, and retain the top n c candidates S i = { S t , t ∈ Sort ( E i )(1 , n c ) } . These selected candidates will be used for the next round of incremental generation. This approach helps identify the most promising results, ensures the quality of the generated text, and enhances generation stability.\n\n3.2.3 Reflective Incremental Generator Algorithm\n\nIn implementing reflective incremental generation, we drew inspiration from the breadth-first search algorithm for trees (Algorithm 1 ).\n\nAlgorithm 1 Reflective Incremental Generator\n\nRequire: Proposed work key element pro , reference paper summaries list refs _ list = [ ref 1 , ref 2 , ...ref n ] , Comparative Summarizer G () , Reflective Evaluator E () , LM decoder for summarization D s , LM decoder for evaluation D e , n_samples for each generation n s , and the number of candidates retained for each turn is n c . S 0 ←{} steps ← len ( refs _ list ) for i = 1 to steps do S′ ←{G(Dg, pro, refi, s, ns), s ∈Si−1}Ei ←E(De, Si′)S i ←{ S t , t ∈ Sort ( E i )(1 , n c ) } end for return S argmax i E n ( i )\n\nnotes: G () corresponds to the Comparative Summarizer function described in § 3.2.1 , and E () corresponds to the Reflective Envaluation function described in § 3.2.2 . At each step, a collection containing n c most promising generated results is maintained, where the depth of the tree equals the number of documents in the relevant literature collection, S ′ contains n c * n s results, while S i − 1 and S i each contain n c results.\n\n4G-Score: LLM-based automaticEvaluation Metrics\n\nThe evaluation of generative tasks has always been challenging. Previous research on literature summarization predominantly depended on text summarization metrics, like ROUGE ( Lin ( 2004a )).\n\nHowever, conventional text summary evaluation metrics such as ROUGE fall short in gauging the quality of literature summaries. It is crucial to adopt more comprehensive evaluation criteria across various dimensions to guarantee that the generated literature summaries align with the necessary standards. Here, inspired by G-Eval ( Liu et al. , 2023 ), we attempted to assess it using LLMs. We established six-dimensional metrics for automatic evaluation based on research on literature summaries ( Justitia and Wang , 2022 ).\n\nEvaluation Steps. We used Large Language Models (LLMs) to score the six dimensions of generic quality and voted for the best summary from a series of model-generated summaries. Specially, to ensure fairness and consistency in evaluation, we simultaneously scored and voted for the generated results of multiple models in a single conversation.\n\nEvaluation Criterion:\n\nConsistency (1-5) : Content consistency between the generated summary and the gold summary. The generated summary must not contain content that conflicts with the gold summary.\n\nCoherence(1-5) : The quality of language coherence in generated summaries, which should not just be a heap of related information.\n\nComparative (1-5) : Assess the extent to whether the generated summary conducts a comparative analysis on references and proposed work. Whether it provides an integrated summary of similar related works.\n\nIntegrity (1-5) : Assess if the summary covers essential elements: research context, reference paper summaries, past research evaluation, contributions, and innovations.\n\nFluency (1-5) : Assess the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n\nCite Accuracy(1-5) : Assess whether the summary correctly cites reference paper in the format ‘[Reference i]’ when mention the reference paper.\n\n5 Experiment\n\nWe validate the capabilities of our proposed ChatCite agent by verifying the following questions: 1) Is the literature summary generated by ChatCite better than that generated directly by LLMs with CoT and other LLM-based literature review approach? 2) Do all the modules in the ChatCite contribute to its effectiveness? 3) What specific impact do the modules in the ChatCite framework have on the quality of generated summary?\n\nIn this section, we conducted a series of experiments to address these questions. Firstly, we introduced our experimental setup (§ 5.1 ). We compared the performance of existing large language models (LLMs) in directly generating related work under zero-shot and few-shot settings, as well as the bestperforming LLM-based literature review approach (§ 5.2 ). Additionally, we performed ablation analysis on each module in our agent to verify their respective capabilities (§ 5.3 ). Finally, we conducted a human study for a detailed quality assessment of the generated related work summaries (§ 5.4 ).\n\n5.1 Experimental Setup\n\nDataset. We conducted experiments to validate on a paper dataset NudtRwG-Citation dataset ( Wang et al. , 2020 ) designed for related work summarization task. This test set includes 50 academic research papers in the field of Computer Science, each data containing the following components: 1) A target paper requiring related work generation without the related work section. 2) A ground truth related work section. 3) Reference papers of the target paper (annotated with authors and years).\n\nEach paper is well-received in conferences of computational linguistics and natural language processing, with an average citation number reaching 63.59, which indicates these target papers are widely recognized by the academic community.\n\nModels. For the LLMs baseline, we employed the GPT-3.5 model ( Ouyang et al. ( 2022 )) with a 16k context window (version gpt-3.5-turbo-1106) and the GPT-4.0 model ( Achiam et al. ( 2023 )) with a 128K context window (gpt-4-turbo-preview). We evaluated their performance under zero-shot and few-shot settings. For the previously bestperforming LLM-based literature review approach, we use the recently proposed approach LitLLM ( Agarwal et al. , 2024 ) as the baseline. We reproduce their ability to generate literature summaries according to the CoT prompt mentioned in their paper. To showcase its best performance, we use GPT-4.0 as the decoder for the LitLLM baseline. For our model, due to the high cost of GPT-4.0, we conducted experiment based on GPT-3.5 (version gpt-3.5-turbo-1106) as the decoder for the experiment. For evaluation, we use GPT-4.0 (gpt-4-turbopreview) as decoder.\n\nImplementation. In zero-shot setting, for GPT3.5 model, due to the limitation of the context window, a two-step approach is used for generation: 1) summarizing and then generating with the prompt [ p s ] = \"Summarize the current article, preserving as much information as possible. Content:{content}\" for summarization. For generating the related work section, we use the prompt [ p g ] = \"Generate the related work section based on the given target paper summary and its references summary. Read the Target Paper Content: {Target}. References content: {References}\" . For GPT-4.0 and LitLLM with GPT-4.0, [ p g ] is directly used for summarization.\n\nIn the few-shot setting, we add the instruction \"Follow the writing style of the example but without including any content from the example. {Examples}\" to the zero-shot prompt.\n\nEvaluation metrics. We utilize both automatic metrics and human evaluations to assess the generic result. We employed traditional automatic metrics for summarization evaluation - the vocabulary overlap measures ROUGE-1/2/L (F1) (Lin (2004b)),our proposed LLM-based evaluation metrics GEval, and human evaluation under the same evaluation criterion.\n\n5.2 Main Results\n\nWe compared the performance of different baseline models on the paper test set (see Table 1 ). In traditional summary evaluation metrics, such as ROUGE, GPT-4.0 achieved the best results under zero-shot settings. Although ROUGE scores of ChatCite may be slightly lower than GPT-4.0 with zero-shot, its performance in quality metrics generated by LLMs and the preference of LLMs is far superior to results obtained directly from other LLM baselines.\n\nSurprisingly, GPT-4.0 performed poorly in fewshot settings.It is found that influenced by examples in the few-shot, resulting in irrelevant and erroneous summaries after case study. Notably, LitLLM with GPT-4.0 produced outcomes similar to GPT-4.0 in zero-shot but significantly lower than ChatCite .\n\nTherefore, we conclude that \"ChatCite performs best among LLM-based literature summarization methods, and the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method.\"\n\nTable 1: Main Results : The results are automatically evaluated using ROUGE-1/2/L (F1) and the GPT-4.0 evaluator. G-Score represents the total score assessed by the GPT-4.0 evaluator, while G-Prf. indicates the model preferences among the five models.\n\n5.3 Ablation Analysis\n\nOur proposed framework can be decomposed into two components: the Key Element Extractor and the Reflective Incremental Generator. The Reflective Incremental Generator comprises two key points: the Comparative Incremental Generation and the Reflective Mechanism. Therefore, we will analyze the three part separately.\n\nKey Element Extractor. To validate the effectiveness of the Key Element Extractor, we chose ChatCite without the Key Element Extractor as a comparison. The ChatCite without Key Element Extractor used the baseline summary prompt [ p s ] to directly summarize the article and then use Reflective Incremental Generator generate the literature summary.\n\nIn Table 2 , comparing the results of ChatCite without Key Element Extractor and ChatCite, we can observe that ChatCite performs better in all dimensions of ROUGE metrics and the metrics generated by the LLM based evaluator. Therefore, it indicates that the Topic Extractor module plays an effective role in literature summarization.\n\nComparative Incremental Mechanism. To validate the effectiveness of the Comparative Incremental Mechanism, we choose ChatCite without Comparative Incremental Mechanism as comparison, following the few-shot baseline prompt [ p s ] and few-shot examples as prompts to directly generate literature summaries from the text after standard summarization. Considering controlling variables for the incremental mechanism, we also incorporated CoT writing instructions into the method to ensure that the experimental results are not influenced by the writing instructions.\n\nIn Table 2 , when comparing ChatCite with and without the Comparative Incremental Mechanism, the results indicate that ChatCite achieves higher ROUGE metrics and LLM-based evaluation metrics compared to ChatCite without the Comparative Incremental Mechanism. This suggests that the Comparative Incremental Mechanism significantly contributes to the effectiveness of literature summarization in the ChatCite framework.\n\nFigure 3: Ablation Study on the Reflective Mechanism. The upper and lower whiskers represent the overall range of the data, while the box displays the distribution of the middle 50% of the dataset, with a line inside the box representing the median of the data. Data points outside the boxplot are considered outliers, indicating data points that significantly deviate from the box and whiskers. It can be observed that ChatCite performs more stable across all dimensions.\n\nReflective Mechanism. In conclusion, we analyzed the reflective mechanism’s impact. G-Scores for various dimensions were assessed based on multiple results from ChatCite , both with and without the Reflective Mechanism. The boxplot results in Figure 3 show similarities between the outcome of ChatCite with and without the Reflective Mechanism. However, the overall results of ChatCite are slightly higher, with minimal distribution outliers, suggesting a more stable generation of results. This affirms that the Reflective Mechanism effectively improves the quality and stability of the text generated in ChatCite .\n\nTable 2: Ablation Results : This table presents the ablation results on the model’s Key Element Extractor and Comparative Incremental Generator, with the results of GPT-3.5 w/few-shot used as the baseline for GPT-3.5.\n\nOverall, through ablation experiments on three components, we have demonstrated that \"each part of ChatCite framework contributes to the improvement of the quality and stability of the generated results in literature summaries\" .\n\n5.4 Human Study\n\nTo conduct a fine-grained analysis on the quality of summary generated by ChatCite and to understand the specific impact of individual modules on summarization, we conducted a human study. Several researchers in the field of computer science, with experience in academic writing, were enlisted to evaluate 10 selected samples using the same set of criteria and choose the better summary.\n\nFigure 4: Human Evaluation vs. G-Score on six dimensions of the generic summary quality. The scoring results of the G-Score model is aligned with the distribution of human evaluations.\n\nFigure 4 demonstrates the results of G-score metric align with human preferences. Specifically, the method incorporating Key Element Extractor exhibits higher content consistency. Summaries generated with the Comparative Incremental generation Mechanism demonstrate better characteristics of literature review, excelling in organizational structure, comparative analysis, and citation accuracy. The fluency of results generated by LLMs is consistently high, with relatively low variation among different models. In terms of human evaluation, summaries generated without the Comparative Incremental Mechanism exhibit overly discrete descriptions for each paper, lacking coherence. Unexpectedly, this feature was not captured in the assessment by the large models.\n\nFigure 5: Human Preference: Average annotator vote distribution for better generated summaries.\n\nAdditionally, Figure 5 shows the extinct human preference of the ChatCite model over the others.\n\n6 Conclusion\n\nLLMs are powerful tools in generating literature summaries, however, it poses the challenges of information omission, lack of comparative summaries and organizational deficiencies. In ChatCite, the Key Element Extractor contributes to improving content consistency, and the Comparative Incremental Generator effectively enhances the organizational structure, comparative analysis, and citation accuracy of the generated summary. Additionally, the literature summaries generated by ChatCite can be directly used for drafting literature reviews. Our study also demonstrated that the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method. In the future, we hope that our work will further inspire research on complex inferential writing, enabling the full potential of LLMs in open-ended writing tasks.\n\nLimitations\n\nIn this work, we focused mainly on the summarization of specific topics based on the selected literatures instead of the collection and the filtering of the literatures themselves. The datasets primarily consist of research articles in the area of computer science and lack research articles from other fields of study to validate our model. Our experimentation used Chat GPT 3.5 as the tool for validating the quality of the generated content and the functionalities of the various components of the agent. We did not explore any additional spec that can influence the result of the GPT3.5 model nor the possibility of using other models as the validation tool. The evaluation of the generated content poses a great challenge. We evaluated the generated results from multiple dimensions using G-Score as the performance metric, but there is still room for improvements over the accuracy of the automatic evaluation process. The generated results exhibit randomness and instability. While our proposed approach demonstrates the effectiveness of the agent, the results have shown further research potential on improving the stability and quality of the output.\n\nEthics Statement\n\nThe dataset we used consists of research articles sourced only from publicly available papers, eliminating concerns about data origin. We employ large language models as generators used and only used for summarizing people’s ideas and literature and never on the innovative writing processes of the academic papers. However, if generated literature summaries are to be incorporated into academic paper writing, a review and editing of the generated results should be conducted. This ensures that academic writing content is free from harmful information and plagiarism issues.\n\nWe will make our code publicly available to ensure experiment reproducibility."
}