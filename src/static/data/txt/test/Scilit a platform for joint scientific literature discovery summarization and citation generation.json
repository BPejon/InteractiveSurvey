{
    "title": "S CI L IT : A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation",
    "authors": "Nianlong Gu Institute of Neuroinformatics, University of Zurich and ETH Zurich nianlong@ini.ethz.ch\n\nRichard H.R. Hahnloser Institute of Neuroinformatics, University of Zurich and ETH Zurich rich@ini.ethz.ch",
    "abstract": "Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes in large and rapidly evolving fields. By making these processes inter-operable, natural language processing (NLP) provides opportunities for creating end-to-end assistive writing tools. We propose S CI L IT , a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. S CI L IT efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https:// scilit.vercel.app",
    "introduction": "When we compose sentences like “Our experiments show that XXX performs significantly worse than YYY” in a manuscript, we may want to find papers that report similar performance evaluations ( Cohan et al. , 2019 ) and discuss these in our manuscript. This process is a non-trivial task requiring in-depth human involvement in finding, summarizing, and citing papers, which raises the question whether it is possible to partly automate this process to reduce users’ cognitive load in searching, retrieving, reading, and rephrasing related findings.\n\nRecent advances in natural language processing (NLP) help answer this question. First, releases of large scientific corpora such as S2ORC ( Lo et al. , 2020 ) and General Index ( Else , 2021 ) provide opportunities for building large databases of scientific papers. Second, such databases can be linked to systems for text retrieval ( Guo et al. , 2020 ), citation recommendation ( Färber and Jatowt , 2020 ; Gu et al. , 2022b ; Medi´ c and Snajder , 2020 ), extractive summarization ( Zhong et al. , 2020 ; Gidiotis and Tsoumakas , 2020 ; Gu et al. , 2022a ), and citation generation ( Xing et al. , 2020a ; Ge et al. , 2021 ; Wang et al. , 2022 ), all of which can be tailored to meet the requirements of an author’s manuscript.\n\nFigure 1: The main workflow of our platform. The example text for the context was selected from Reimers and Gurevych ( 2019 ).\n\nTo build a comprehensive system that helps authors with finding, reading, and summarizing of literature, the following challenges must be overcome: The system must index many papers (e.g., S2ORC has over 136 million papers ( Lo et al. , 2020 )) to achieve good coverage, it must respond quickly to queries, and it must be flexible to handle database additions and deletions. In addition, the overall architecture should be modular so that components can be easily upgraded when better algorithms become available.\n\nTo meet these challenges, we developed S CI L IT , a platform for literature discovery, summarization, and citation generation. We propose a hierarchical architecture for paper retrieval that efficiently retrieves papers from multiple large corpora. On each corpus (e.g., S2ORC and PMCOA ( of Medicine , 2003 )), we build an efficient prefetching system based on a keyword inverted index and a document embedding index. The prefetched documents are then re-ordered (re-ranked) by a fine-tuned SciBERT ( Beltagy et al. , 2019 ). Such an architecture allows us to dynamically add and remove databases and update one database and its index without significantly affecting the others. From a retrieved document (i.e., target paper), we extract highlights using a light-weight extractive summarization model proposed in Gu et al. ( 2022a ). Furthermore, using a fine-tuned T5 model ( Raffel et al. , 2020 ), we generate a citing sentence based on the abstract of the target paper, the context (the text surrounding the original citation sentence), and the keywords provided by a user. We also develop a microservicebased architecture that allows easy updating of algorithms.\n\nIn summary, our main contributions are:\n\nWe demonstrate S CI L IT , a platform for searching, summarizing, and citing scientific papers. • We evaluate S CI L IT on scientific literature retrieval, paper summarization, and contextaware citation sentence generation, and showcase the generation of a related-work paragraph.\n\nA live demo website of our system is at https://scilit.vercel.app and our implementation and data are at https://github.com/nianlonggu/ SciLit and a video demonstrating the system can be viewed at https://youtu.be/PKvNaY5Og1Y",
    "main_content": "S CI L IT : A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation\n\nNianlong Gu Institute of Neuroinformatics, University of Zurich and ETH Zurich nianlong@ini.ethz.ch\n\nRichard H.R. Hahnloser Institute of Neuroinformatics, University of Zurich and ETH Zurich rich@ini.ethz.ch\n\nAbstract\n\nScientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes in large and rapidly evolving fields. By making these processes inter-operable, natural language processing (NLP) provides opportunities for creating end-to-end assistive writing tools. We propose S CI L IT , a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. S CI L IT efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https:// scilit.vercel.app\n\n1 Introduction\n\nWhen we compose sentences like “Our experiments show that XXX performs significantly worse than YYY” in a manuscript, we may want to find papers that report similar performance evaluations ( Cohan et al. , 2019 ) and discuss these in our manuscript. This process is a non-trivial task requiring in-depth human involvement in finding, summarizing, and citing papers, which raises the question whether it is possible to partly automate this process to reduce users’ cognitive load in searching, retrieving, reading, and rephrasing related findings.\n\nRecent advances in natural language processing (NLP) help answer this question. First, releases of large scientific corpora such as S2ORC ( Lo et al. , 2020 ) and General Index ( Else , 2021 ) provide opportunities for building large databases of scientific papers. Second, such databases can be linked to systems for text retrieval ( Guo et al. , 2020 ), citation recommendation ( Färber and Jatowt , 2020 ; Gu et al. , 2022b ; Medi´ c and Snajder , 2020 ), extractive summarization ( Zhong et al. , 2020 ; Gidiotis and Tsoumakas , 2020 ; Gu et al. , 2022a ), and citation generation ( Xing et al. , 2020a ; Ge et al. , 2021 ; Wang et al. , 2022 ), all of which can be tailored to meet the requirements of an author’s manuscript.\n\nFigure 1: The main workflow of our platform. The example text for the context was selected from Reimers and Gurevych ( 2019 ).\n\nTo build a comprehensive system that helps authors with finding, reading, and summarizing of literature, the following challenges must be overcome: The system must index many papers (e.g., S2ORC has over 136 million papers ( Lo et al. , 2020 )) to achieve good coverage, it must respond quickly to queries, and it must be flexible to handle database additions and deletions. In addition, the overall architecture should be modular so that components can be easily upgraded when better algorithms become available.\n\nTo meet these challenges, we developed S CI L IT , a platform for literature discovery, summarization, and citation generation. We propose a hierarchical architecture for paper retrieval that efficiently retrieves papers from multiple large corpora. On each corpus (e.g., S2ORC and PMCOA ( of Medicine , 2003 )), we build an efficient prefetching system based on a keyword inverted index and a document embedding index. The prefetched documents are then re-ordered (re-ranked) by a fine-tuned SciBERT ( Beltagy et al. , 2019 ). Such an architecture allows us to dynamically add and remove databases and update one database and its index without significantly affecting the others. From a retrieved document (i.e., target paper), we extract highlights using a light-weight extractive summarization model proposed in Gu et al. ( 2022a ). Furthermore, using a fine-tuned T5 model ( Raffel et al. , 2020 ), we generate a citing sentence based on the abstract of the target paper, the context (the text surrounding the original citation sentence), and the keywords provided by a user. We also develop a microservicebased architecture that allows easy updating of algorithms.\n\nIn summary, our main contributions are:\n\nWe demonstrate S CI L IT , a platform for searching, summarizing, and citing scientific papers. • We evaluate S CI L IT on scientific literature retrieval, paper summarization, and contextaware citation sentence generation, and showcase the generation of a related-work paragraph.\n\nA live demo website of our system is at https://scilit.vercel.app and our implementation and data are at https://github.com/nianlonggu/ SciLit and a video demonstrating the system can be viewed at https://youtu.be/PKvNaY5Og1Y\n\n2 SciLit\n\nFigure 1 shows the workflow of our system. A literature discovery module receives a context (a text) and keywords provided by a user and recommends a list of relevant papers that are semantically similar with the context and that match the keywords used as a Boolean filter ( Gökçe et al. , 2020 ). For each recommended paper, an extractive summarizer selects a short list of sentences from the full text as highlights. From the target paper selected by the user, a citation generation module takes the abstract together with the context and keywords as inputs and generates a citation sentence that references the target paper and that fits the context and keywords.\n\nFigure 2: Schematic of literature discovery (paper retrieval). From each database, candidate documents are prefetched by a cascade of keyword Boolean filter and embedding-based nearest neighbor search. Then, candidate documents are reranked by a fine-tuned SciBERT.\n\nWe define the context as the text before a citation sentence because we focus on the workflow of first finding papers (i.e., the missing citation as in as in Gu et al. ( 2022b ); Medi´ c and Snajder ( 2020 )) and then writing citation sentences. The user-provided keywords are optional. When no keywords are explicitly given during training and evaluation of our system, we extract keywords from the context, the cited paper, and the citation sentence as substitutes.\n\n2.1 Literature Discovery\n\nThe literature discovery module takes as inputs the context and keywords and recommends papers that are worth citing in the context. To strike a balance between query accuracy and speed on large scientific corpora, our document discovery module uses a two-stage prefetching-ranking strategy ( Gu et al. , 2022b ) (Figure 2 ): For each scientific corpus, we build a database and create an efficient prefetching model that we use to pre-filter a number N p (see the discussion of N p in Table 2 and Section 3.2 ) of candidate documents from each corpus based on the provided keywords and context. After removing duplicates, we re-rank all prefetched documents using a trained reranker.\n\nDatabases. We dump each corpus into a separate SQLite ( Hipp , 2000 ) database to allow flexibility in deploying and maintaining of independent prefetching servers. We further process documents from different corpora into a unified JSON schema compatible with a single codebase for indexing, querying, summarizing, and displaying documents from different corpora. The JSON schema includes keys “Title”, “Author”, etc., for parsed metadata, and “Content.Abstract_Parsed”, “Content.Fullbody_Parsed” for parsed full text, The details are given in Appendix B .\n\nPrefetching. The prefetching model of a given SQLite database consists of an inverted index and an embedding index. The inverted index stores the paper IDs (unique identifiers for retrieving a paper’s content) of all publications that contain a given keyword, e.g., a unigram such as “computer” or a bigram such as “machine learning”. The embedding index is a table of embeddings of all papers in the database. Embeddings are 256-dimensional vectors computed by Sent2Vec ( Pagliardini et al. , 2018 ) (we simply average the embeddings of all words in a document). We trained Sent2Vec on sentences from the full-text papers contained in S2ORC.\n\nTable 1: Statistics of our literature discovery system. We indexed S2ORC ( Lo et al. , 2020 ), PMCOA ( of Medicine , 2003 ), and arXiv ( Kaggle , 2022 ), which contain large numbers of recent scientific papers in diverse fields.\n\nUsing the keywords and a specific syntax, we first perform Boolean filtering ( Gökçe et al. , 2020 ) of the inverted index. For example, given “POS tag;2010..2022”, we filter papers published between 2010 and 2022 that mention “POS tag”. The filtered papers are then ranked based on the cosine similarity between the papers’ Sent2Vec embeddings and the context embedding. Such a hybrid of lexical filtering and semantic ranking allows users to find papers that are semantically similar to the context and that flexibly meet a constrained search scope.\n\nStatistics for the database and indexing system are reported in Table 1 . Details of the indexing implementation are shown in Appendix C .\n\nDuplicate Removal. Since corpora can overlap, the prefetched candidates from multiple corpora can contain duplicate items. To remove duplicated candidates, we check the title and authors and keep only one record per paper for reranking.\n\nReranking. We use SciBERT ( Beltagy et al. , 2019 ) to rerank prefetched candidate papers, aiming at highly ranking candidates that can be cited given the context and keywords. We follow Gu et al. ( 2022b ) to compute an affinity score as follows: we pass an input text “[CLS] query [PAD] paper [PAD]” to SciBERT, where the query q is a concatenation of the context and the keywords, and paper d is a concatenation of the title and the abstract of the candidate paper. The encoded output of the “[CLS]” token is passed to a linear layer, which outputs a scalar s ( q, d ) that we interpret as the affinity score between the query q and the paper d . To train the reranker, we use the cross-entropy loss:\n\nwhere d + is the paper actually cited in the query, and d − is one of N ( N = 10) uncited papers that are randomly sampled from prefetched candidates at each training iteration.\n\n2.2 Extractive Summarization\n\nThe extractive summarization module extracts a short list of sentences from the full text of a paper to highlight the main points to a reader. We choose the summary to be extractive rather than abstractive to prevent readers from being misled by potential hallucinations introduced by abstractive summarization models ( Nan et al. , 2021 ; Xu et al. , 2020 ; Wang et al. , 2020 ). The extractive summarization model must efficiently select sentences from a given document so that users do not experience obvious delays.\n\nIn this paper, we employ MemSum, an RNNbased extractive summarizer that models the extraction process as a Markov decision process in a reinforcement learning framework. MemSum has been trained on the PubMed dataset Gu et al. ( 2022a ) and it can summarize long papers without exhausting GPU memory due to its lightweight model structure. Also, MemSum is computationally efficient, taking only 0.1 sec on average to summarize a paper. These features make it a suitable model for our extractive summarization module.\n\n2.3 Citation Generation Module\n\nThe citation generation module acts as an abstract summarizer that takes as input the context, the keywords, and the target paper to be cited; it then generates a sentence that cites the target paper and narrates it in context.\n\nBy providing keywords as inputs to a sequenceto-sequence model, our input differs from previous works on automatic citation generation ( Ge et al. , 2021 ; Xing et al. , 2020b ), which use only the context as inputs but no keywords. We consider keywords to be an important source of input because we believe that authors usually have a clear intention when citing a paper, and a keyword can sometimes more easily convey this intention than a long text. In the case shown in Figure 1 , for example, after writing the context “MAX pooling performs worse than MEAN pooling”, the author naturally intends to discuss papers about “MAX pooling”. Therefore, the keyword “MAX pooling” should be used as a thematic cue for citation sentence generation. Moreover, making the citation generation model conditional on keywords also allows users to fine-tune the generated citation text by simply adjusting the keywords, thus making the system interactive and conveniently tunable.\n\nTo make the generation conditional on context, keywords, and cited papers, we fine-tuned a T5 ( Raffel et al. , 2020 ) so that its input is a concatenation of three attributes: keywords, context, and the abstract of a cited paper, each preceded by a special field name to make attributes distinguishable to the model: keywords: XXX. context: XXX. target abstract: XXX. The corresponding decoding output is the actual citation sentence that cites the target paper.\n\n2.4 Microservice-based Architecture\n\nWe built our platform as a network of microservices (Figure 3 ). An API gateway routes requests from the frontend to the target microservice on the backend. The microservices run separate modules on their respective Flask servers ( Aggarwal , 2014 ) and communicate with each other by sending HTTP requests and waiting for responses. When a query request arrives, the API gateway forwards the query to the literature discovery service, which calls the prefetching and reranking services to get the reranked IDs. The API gateway then sends the paper IDs to the extractive summarization service to receive the highlights of each recommended paper. The gateway also sends the context, keywords, and recommended paper IDs to the citation generation service to suggest citation sentences. The database interface service manages the databases of multiple scientific corpora and provides a unified interface to access the paper content given its ID. Each microservice runs in an independent environment, which makes it easy to upgrade backend systems online, such as adding or removing a database or updating an algorithm.\n\nFigure 3: The architecture of our platform. The direction of an arrow represents the direction of data flow.\n\n3 Evaluation\n\nIn this section, we first show how S CI L IT works and then we evaluate its performance.\n\n3.1 Demonstration\n\nOur user interface runs on a web page (Figure 4 ) created with ReactJS 1 . The left sidebar is an input panel where users can enter context and keywords and trigger a query by clicking the search button. Retrieved papers are displayed in the search-results panel on the right. Users can scroll up and down or paginate to browse through the recommended papers. Each paper is accompanied by highlights and a suggested citation sentence generated by our extractive summarization and citation generation services, respectively. Users can cite a paper by clicking on the cite button and the suggested citation sentence will jump to the editing area on the left where users can tweak the sentence by changing keywords and clicking on the fine-tune generation button, or they can edit the sentences manually. Exporting citation information is also supported.\n\nFigure 4: Overview of the user interface. The context text comes from the related work section in Glass et al. ( 2020 ).\n\nTable 2: Paper retrieval performance measured as the recall of the top K recommendations. N p denotes the number of prefetched candidates per corpus.\n\n3.2 Performance\n\nEvaluation Dataset. We evaluated S CI L IT on a test set containing 1530 samples, which are mainly papers published in 2022 in the fields of computer and biomedical sciences. Each sample contains the following information: 1) context, up to 6 sentences preceding the citation sentence and within the same section; 2) keywords, up to 2 uni- or bigrams that occur in all of the context, the citation sentence, and the cited paper; 3) ID of the cited paper; 4) the citation sentence following the context, which is the ground truth for evaluating generated citations. For quality control, we only include citation sentences in the test set that cite one paper.\n\nPaper Retrieval. For each sample in the evaluation dataset, we use context and keywords as queries and invoke the literature search service to first prefetch N p candidates from each of the three corpora (S2ORC, PMCOA, and arXiv). We remove duplicates and then we rank the prefetched candidates. The top K recommendations serve to evaluate the retrieval performance (Table 2 ). We observed that for large K ( K = 50 , 100) , the recall increases as N p increases, whereas for small K ( K = 5 , 10 , 20) , the recall first increases and then starts to decrease, indicating that the reranking performance is impacted by increasing number of prefetched candidates. We choose N p = 100 as the default value, which gives rise to fast reranking and achieved the best performance at R@10.\n\nTable 3: The extractive summarization performance. “*\" indicates statistical significance in comparison to baselines with a 95% bootstrap confidence interval.\n\nExtractive Summarization. To evaluate the summaries, following Zhong et al. ( 2020 ); Xiao and Carenini ( 2019 ), we computed the ROUGE F1 scores between the summary sentences extracted from the full body and the corresponding abstract. MemSum significantly outperformed BertSum ( Liu , 2019 ), a Bert-based summarizer that requires truncation of long documents, indicating the effectiveness of MemSum in extractively summarizing scientific documents.\n\nTable 4: The citation generation performance.\n\nTable 5: Ablation study on retrieval and citation generation performance.\n\nCitation Generation. We assessed our joint retrieval and citation generation system by letting it recommend papers based on context and keywords first, then generate K citation sentences corresponding to each of the top K suggested papers. Next, we calculate the ROUGE F1 score by comparing each generated citation sentence with the ground-truth citation sentence (the sentence that actually follows the context text in the test example), keeping track of the highest ROUGE F1 score in this set. This method is named the \"Best-of-topK \" procedure for ease of understanding.\n\nWe hypothesized that generating multiple citation sentences, one for each topK paper, would increase the chances of crafting a suitable citation sentence. To verify this, we compared the effectiveness of the \"Best-of-topK \" approach with a \"generation-only\" system. The latter generates a single citation sentence based on the actual cited paper provided as ground truth.\n\nWe observed that for K = 5 and 10 , the “Bestof-topK ” pipeline achieved significantly higher ROUGE scores than the \"generation only\" pipeline (Table 4 ), indicating that the paper retrieval module contributes positively to the citation generation process and increases the chance of suggesting appropriate citation sentences. We believe that this result further supports our idea of developing an integrated system for joint retrieval and generation.\n\n3.3 Ablation Study\n\nTo analyze the impact of keywords, we evaluated retrieval and generation systems without keywords. For document retrieval, we first prefetched N p = 100 candidates from each corpus and then ranked them based on context only. For citation generation, we trained a T5 model to learn to generate citation sentences with only the context and the title and abstract of the cited paper and evaluated it on the evaluation set. We observe a significant degradation in the performance of literature retrieval and citation generation (Table 5 ), which demonstrates the utility of keywords for recommending relevant papers and generating accurate citations on our platform.\n\n4 Related Work\n\nRecently, AI-driven platforms focused on literature recommendation and scientific paper summarization have been proposed. (keywords: platform , paper: #2) One such platform is AI Research Navigator ( Fadaee et al. , 2020 ), which combines classical keyword search with neural retrieval to discover and organize relevant literature. (keywords: scientific; summarization; platform , paper #3) Another platform is Anne O’Tate, which supports user-driven summarization, drill-down and mining of search results from PubMed, the leading search engine for biomedical literature ( Smalheiser et al. , 2021 ). (keywords: related work generation , paper #9) Chen and Zhuge ( 2019 ) automatically generates related work by comparing the main text of the paper being written with the citations of other papers that cite the same references.\n\nIn the previous paragraph, the italicized citation sentences are generated by S CI L IT . For generating a sentence, it used all preceding sentences in the paragraph as contexts and the keywords in parentheses. We browsed the 100 recommended papers by turning pages and reading the corresponding citation sentences (as shown in Figure 4 ). The numbers in parentheses indicate the ranks of the recommended papers.\n\n5 Conclusion and Future Work\n\nThis paper demonstrates S CI L IT , a platform for joint scientific literature retrieval, paper summarization, and citation generation. S CI L IT can efficiently recommend papers from hundreds of millions of papers and proactively provide highlights and suggested citations to assist authors in reading and discussing the scientific literature. In addition, our prefetching, reranking, and citation generation system can be conditioned on user-provided keywords, which provides flexibility and adjusts the platform’s response to user intention. In the future, we will further improve the performance of each module, especially the citation generation part, and collect feedback from users to improve the overall workflow and the frontend user experience.\n\nAcknowledgements\n\nThis project was supported by the Open Research Data Program of the ETH Board (ORD2000103). We thank the anonymous reviewers for their useful comments."
}