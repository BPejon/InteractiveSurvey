{
    "title": "Enabling Large Language Models to Generate Text with Citations",
    "authors": "Tianyu Gao Howard Yen Jiatong Yu Danqi Chen\n\nDepartment of Computer Science & Princeton Language and Intelligence Princeton University {tianyug,hyen,jiatongy,danqic}@cs.princeton.edu",
    "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations , improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE , the first benchmark for A utomatic L LMs’ C itation E valuation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
    "introduction": "Large language models (LLMs; Brown et al. , 2020 ; OpenAI , 2023 ) have gained increasing popularity as a tool for information seeking. While they generate engaging and coherent responses, their outputs are prone to hallucination and often contain factually incorrect information ( Ji et al. , 2023 ). This makes it harder for users to trust and verify LLMgenerated outputs without any supporting evidence. In this work, we study a new generation paradigm for LLMs, in which we require LLMs to provide citations to one or a few text passages for any statement they generate (Figure 1 ). Incorporating citations brings several benefits: (1) users can easily verify LLMs’ claims with the provided citations; (2) LLMs can generate text that faithfully follows cited passages, which has the promise to improve correctness and alleviate hallucination.\n\nFigure 1: The task setup of ALCE. Given a question, the system generates text while providing citing passages from a large retrieval corpus. Each statement may contain multiple citations (e.g., [1][2] ).\n\nMultiple commercial systems have adopted this paradigm: Bing Chat 2 and perplexity.ai 3 respond to user questions in natural language with references to Web pages. Nakano et al. ( 2021 ); Menick et al. ( 2022 ) share a similar motivation, but they mainly experiment with commercial search engines and closed-source models, making their results difficult to evaluate. Retrieval-augmented LMs ( Borgeaud et al. , 2022 ; Izacard et al. , 2022 ) incorporate retrieved passages during both training and inference, but do not guarantee faithfulness to retrieved passages or explicitly provide citations. Additionally, previous studies mostly rely on human evaluation ( Nakano et al. , 2021 ; Menick et al. , 2022 ; Liu et al. , 2023 ), which is expensive and difficult to reproduce. We argue that the absence of automated evaluation hinders the advances of such systems.\n\nTable 1: The three datasets used in our ALCE benchmark. These datasets cover a wide range of question types and the corresponding corpora span from Wikipedia to Web-scale document collection.\n\nWe present ALCE , the first reproducible benchmark for automatically evaluating LLMs’ generations with citations. ALCE assumes a naturallanguage question and a retrieval corpus, and requires building end-to-end systems to retrieve relevant passages from the corpus, generate a response to the question, and cite corresponding supporting passages. We compile three datasets that cover different types of questions and corpora— ASQA ( Stelmakh et al. , 2022 ), QAMPARI ( Rubin et al. , 2022 ), and ELI5 ( Fan et al. , 2019 )—as shown in Table 1 . Different from previous benchmarks ( Lee et al. , 2019 ; Bohnet et al. , 2022 ), ALCE evaluates long-text generation, focusing on automatically evaluating citation quality, and allows citing multiple passages for individual statements.\n\nWe design automatic evaluation methods in three dimensions: fluency , correctness , and citation quality . Specifically, we use MAUVE ( Pillutla et al. , 2021 ) to measure fluency, propose tailored correctness metrics for each dataset, and adopt a natural language inference (NLI) model ( Honovich et al. , 2022 ) to measure citation quality. We showcase how the three dimensions together contribute to a robust evaluation, preventing systems from exploiting shortcuts. Additionally, we conduct human evaluation and demonstrate a strong correlation with our automatic metrics.\n\nWe experiment on multiple systems with stateof-the-art LLMs and retrievers and also propose novel prompting strategies to synthesize retrieved text into text generation. Although all systems are capable of providing fluent and coherent responses, there remains substantial room for improvement in terms of correctness and citation quality: For example, on the ELI5 dataset, around 50% generations of our ChatGPT and GPT-4 baselines are not fully supported by the cited passages. Additionally, we find that (1) a closed-book model (generating answers without accessing any retrieved documents)\n\nwith post-hoc citing achieves good correctness but much worse citation quality; (2) although interactive retrieval approaches ( Yao et al. , 2023 ; Schick et al. , 2023 ) offer more flexibility in when/what to retrieve, they do not improve the performance on this challenging benchmark; (3) summarizing the retrieved passages in a shorter text improves correctness but not citation quality; (4) reranking multiple generations boosts citation quality measured by human evaluation; (5) incorporating more retrieved passages in context does not help ChatGPT but improves GPT-4 performance.\n\nOur extensive analyses highlight three major challenges of building LLMs to generate text with citations: (1) the retrieval quality is crucial to the final performance and has substantial room for improvement; (2) LLMs’ limited context window restricts the number of passages they can incorporate; (3) current LLMs struggle to synthesize multiple documents in context without being distracted by irrelevant ones, although better instruction tuning brings significant improvement. These challenges pose promising research directions for developing better systems integrating retrieval and LLMs.",
    "main_content": "Enabling Large Language Models to Generate Text with Citations\n\nTianyu Gao Howard Yen Jiatong Yu Danqi Chen\n\nDepartment of Computer Science & Princeton Language and Intelligence Princeton University {tianyug,hyen,jiatongy,danqic}@cs.princeton.edu\n\nAbstract\n\nLarge language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations , improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE , the first benchmark for A utomatic L LMs’ C itation E valuation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.\n\n1 Introduction\n\nLarge language models (LLMs; Brown et al. , 2020 ; OpenAI , 2023 ) have gained increasing popularity as a tool for information seeking. While they generate engaging and coherent responses, their outputs are prone to hallucination and often contain factually incorrect information ( Ji et al. , 2023 ). This makes it harder for users to trust and verify LLMgenerated outputs without any supporting evidence. In this work, we study a new generation paradigm for LLMs, in which we require LLMs to provide citations to one or a few text passages for any statement they generate (Figure 1 ). Incorporating citations brings several benefits: (1) users can easily verify LLMs’ claims with the provided citations; (2) LLMs can generate text that faithfully follows cited passages, which has the promise to improve correctness and alleviate hallucination.\n\nFigure 1: The task setup of ALCE. Given a question, the system generates text while providing citing passages from a large retrieval corpus. Each statement may contain multiple citations (e.g., [1][2] ).\n\nMultiple commercial systems have adopted this paradigm: Bing Chat 2 and perplexity.ai 3 respond to user questions in natural language with references to Web pages. Nakano et al. ( 2021 ); Menick et al. ( 2022 ) share a similar motivation, but they mainly experiment with commercial search engines and closed-source models, making their results difficult to evaluate. Retrieval-augmented LMs ( Borgeaud et al. , 2022 ; Izacard et al. , 2022 ) incorporate retrieved passages during both training and inference, but do not guarantee faithfulness to retrieved passages or explicitly provide citations. Additionally, previous studies mostly rely on human evaluation ( Nakano et al. , 2021 ; Menick et al. , 2022 ; Liu et al. , 2023 ), which is expensive and difficult to reproduce. We argue that the absence of automated evaluation hinders the advances of such systems.\n\nTable 1: The three datasets used in our ALCE benchmark. These datasets cover a wide range of question types and the corresponding corpora span from Wikipedia to Web-scale document collection.\n\nWe present ALCE , the first reproducible benchmark for automatically evaluating LLMs’ generations with citations. ALCE assumes a naturallanguage question and a retrieval corpus, and requires building end-to-end systems to retrieve relevant passages from the corpus, generate a response to the question, and cite corresponding supporting passages. We compile three datasets that cover different types of questions and corpora— ASQA ( Stelmakh et al. , 2022 ), QAMPARI ( Rubin et al. , 2022 ), and ELI5 ( Fan et al. , 2019 )—as shown in Table 1 . Different from previous benchmarks ( Lee et al. , 2019 ; Bohnet et al. , 2022 ), ALCE evaluates long-text generation, focusing on automatically evaluating citation quality, and allows citing multiple passages for individual statements.\n\nWe design automatic evaluation methods in three dimensions: fluency , correctness , and citation quality . Specifically, we use MAUVE ( Pillutla et al. , 2021 ) to measure fluency, propose tailored correctness metrics for each dataset, and adopt a natural language inference (NLI) model ( Honovich et al. , 2022 ) to measure citation quality. We showcase how the three dimensions together contribute to a robust evaluation, preventing systems from exploiting shortcuts. Additionally, we conduct human evaluation and demonstrate a strong correlation with our automatic metrics.\n\nWe experiment on multiple systems with stateof-the-art LLMs and retrievers and also propose novel prompting strategies to synthesize retrieved text into text generation. Although all systems are capable of providing fluent and coherent responses, there remains substantial room for improvement in terms of correctness and citation quality: For example, on the ELI5 dataset, around 50% generations of our ChatGPT and GPT-4 baselines are not fully supported by the cited passages. Additionally, we find that (1) a closed-book model (generating answers without accessing any retrieved documents)\n\nwith post-hoc citing achieves good correctness but much worse citation quality; (2) although interactive retrieval approaches ( Yao et al. , 2023 ; Schick et al. , 2023 ) offer more flexibility in when/what to retrieve, they do not improve the performance on this challenging benchmark; (3) summarizing the retrieved passages in a shorter text improves correctness but not citation quality; (4) reranking multiple generations boosts citation quality measured by human evaluation; (5) incorporating more retrieved passages in context does not help ChatGPT but improves GPT-4 performance.\n\nOur extensive analyses highlight three major challenges of building LLMs to generate text with citations: (1) the retrieval quality is crucial to the final performance and has substantial room for improvement; (2) LLMs’ limited context window restricts the number of passages they can incorporate; (3) current LLMs struggle to synthesize multiple documents in context without being distracted by irrelevant ones, although better instruction tuning brings significant improvement. These challenges pose promising research directions for developing better systems integrating retrieval and LLMs.\n\n2 Task Setup and Datasets\n\nOur task is formalized as follows: Given a query q and a corpus of text passages D , the system is required to return an output of n statements s 1 , ..., s n , and each statement S , which consists s i cites a list of passages C i = { c i, , c i, , . . . } 4 , where c ∈D . In this work, we segment LLMs’ output into statements by sentence boundaries. While LLMs may include sentences that do not require a citation, such as “ I’m happy to help ”, we observe that almost all sentences that LLMs output provide valuable information and require citations, similar to findings in Liu et al. ( 2023 ). In this work, citations are enclosed by box brackets such as [1][2] .\n\nfollowing previous works on open-domain question We divide the corpus D into 100-word passages answering ( Karpukhin et al. , 2020 ; Petroni et al. , 2021 ; Piktus et al. , 2021 ), in contrast to commercial systems like Bing Chat, which cite entire Web pages. We take 100-word passages because it is easier for humans to verify, and allows for more retrieved passages to fit in LLMs’ limited context.\n\nWe choose QA datasets so that (1) they contain factual questions, in which references are important; (2) questions require long-text answers that cover multiple aspects; (3) answering the questions requires synthesizing multiple sources. We select three datasets (Table 1 ) and introduce them below. See § B for additional statistics.\n\nASQA ( Stelmakh et al. , 2022 ) is a long-form factoid dataset. As shown in Figure 1 , each question is an ambiguous question from AmbigQA ( Min et al. , 2020 ) that requires multiple short answers to cover different aspects, and the dataset provides a longform answer that covers all short answers. Since most questions can be answered by Wikipedia, we use the 2018-12-20 Wikipedia snapshot as D .\n\nQAMPARI ( Rubin et al. , 2022 ) is a factoid QA dataset constructed from Wikipedia, where the answer is a list of entities that are drawn from different passages. Same as ASQA, we use the 2018-12- 20 Wikipedia as the corpus.\n\nELI5 ( Fan et al. , 2019 ) is a long-form QA dataset built on the Reddit forum “Explain Like I’m Five”. Most ELI5 questions are how/why/what questions that require long answers and multiple passages as evidence. Due to the diverse topics discussed in the questions, we use Sphere ( Piktus et al. , 2021 )—a filtered version of Common Crawl 7 —as the corpus. The ELI5 dataset is widely used in related work due to its challenging nature ( Nakano et al. , 2021 ; Menick et al. , 2022 ; Liu et al. , 2023 ).\n\nWe randomly select 1,000 examples from the development set of each dataset for ALCE. Our benchmark primarily assesses the citation capabilities of existing LLMs and does not provide training data, as there are no available examples that provide supervision for citations in these datasets.\n\n3 Automatic Evaluation\n\nOur benchmark measures the following three dimensions of system responses:\n\nFluency : whether the model’s generated text is fluent and coherent. • Correctness : whether the answer is accurate and covers all aspects of interest. • Citation quality : whether the answer is well supported by the cited passages and no irrelevant passages are cited.\n\nIn the following, we present automatic metrics for each dimension and discuss why the combination of the three metrics provides a robust evaluation.\n\n3.1 Fluency\n\nWe use MAUVE ( Pillutla et al. , 2021 ) to evaluate the fluency of the output (§ C ). We deploy MAUVE for ASQA and ELI5 and omit it for QAMPARI, as QAMPARI only requires a list of short answers as the response and LLMs consistently adhere to the format in our experiments. As MAUVE is sensitive to output length and text style, and most LLMs are capable of producing fluent text, we mainly employ it as a sanity check as long as the MAUVE scores are high enough.\n\n3.2 Correctness\n\nOur objective is to measure the informativeness and utility of the generation to the question. Liu et al. ( 2023 ) propose to directly evaluate perceived utility by humans, a process difficult to automate. Therefore, we use correctness—whether the response is accurate compared to a ground truth answer—as a proxy. Evaluating the correctness of long-form generation is a challenging task ( Krishna et al. , 2021 ), and we describe our strategy for each dataset below. Figure 2 illustrates the metrics and we include additional implementation details in § C .\n\nFor ASQA, we follow Stelmakh et al. (2022)and calculate the recall of correct short answers by checking whether the short answers (provided by the dataset) are exact substrings of the generation ( exact match recall ; EM recall).\n\nFor QAMPARI, we follow Rubin et al. (2022)and calculate the precision and recall of the model prediction, by checking the exact match to the gold answer list. We add one additional adjustment: considering that users often want to know only a few example answers of the question, our evaluation considers recall to be 100% if the prediction includes at least 5 correct answers ( recall-5 ).\n\nFigure 2: Evaluation of correctness (details in § 3.2 ).\n\nUnlike ASQA and QAMPARI, the ELI5 dataset does not provide short entity answers. Fan et al. ( 2019 ) use ROUGE for evaluation, which does not reflect the correctness well ( Krishna et al. , 2021 ; § A ). Inspired by works in summarization evaluation ( Zhang and Bansal , 2021 ; Kamoi et al. , 2023 ; Wang et al. , 2020 ), we use InstructGPT ( text-davinci-003 ; Ouyang et al. , 2022 ) to generate three “sub-claims”. Then we use TRUE 8 ( Honovich et al. , 2022 ), a T5-11B ( Raf- fel et al. , 2020 ) model fine-tuned on a collection of natural language inference (NLI) datasets, to check whether the model output entails the sub-claims ( claim recall ). TRUE targets factual correctness and has been used by previous works in similar context ( Bohnet et al. , 2022 ; Gao et al. , 2023 ). We demonstrate that claim recall provides a more accurate measure of correctness than existing metrics (more details in § A ).\n\n3.3 Citation Quality\n\nWe evaluate citation qualities using two metrics: (1) citation recall , which determines if the output is entirely supported by cited passages, and (2) citation precision , which identifies any irrelevant citations. Although we prioritize citation recall as it entails a well-supported and truthful answer, enhancing precision is crucial for better user satisfaction, reducing the need for human review of extraneous passages. Figure 3 provides an illustrated example. We use the NLI model TRUE ( Honovich et al. , 2022 ) again to automatically examine whether the cited passages entail the model generation. We conduct human evaluation (§ 6 ) to demonstrate strong human correlation of our metric.\n\nFigure 3: Evaluation of citation quality (details in § 3.3 ). We use an NLI model to verify whether a statement is supported by its citations.\n\nCitation recall. We calculate the citation recall of each statement (0 or 1) and average over all statements in the model response. For each statement s i , its citation recall is 1 if and only if there is at least one citation ( C i ̸ = ∅ ) and ϕ ( concat ( C i ) , s i ) = 1 , where ϕ ( premise , hypothesis ) is the NLI model that outputs 1 if the premise entails the hypothesis, and 0 otherwise; concat ( C i ) concatenates all passages in C i together (details in § C ). The NLI evaluation is in accordance with the attributable to identified sources (AIS) framework ( Rashkin et al. , 2023 ): ϕ ( concat ( C i ) , s i ) = 1 implies that s i is true based solely on concat ( C i ) .\n\nCitation precision. Our citation precision evaluation detects citations that are irrelevant, but it does not require citing a minimal set. We follow this design because human writing often cites redundant sources to enhance credibility; human readers may also appreciate multiple citations, especially when it pertains to critical claims such as medical advice.\n\nWe calculate the citation precision for each citation (0 or 1) and average over all citations in the Table 2: An example of our V ANILLA method. Different colors represent prompt , model generation , and\n\nresponse. We first define if a citation is “irrelevant”. Intuitively, a citation c i,j is “irrelevant” if (a) c i,j itself cannot support s i and (b) removing c i,j does not affect the rest of the citations to support s i . Formally, c i,j is “irrelevant” if and only if\n\n(a) ϕ ( c i,j , s i ) = 0 , AND (b) ϕ ( concat ( C i \\ { c } ) , s i ) = 1 .\n\nc i,j has a precision of 1 if s i has recall =1 and c i,j is not irrelevant. For example (Figure 3 ), when s 3 cites three references [2][4][5] and recall =1 , [2] is “irrelevant” if ϕ ( [2] , s 3 ) = 0 and ϕ ( [4][5] , s 3 ) = 1 . For condition (b) to work, we set recall =1 as a prerequisite for precision = 1 . Note that this algorithm overlooks the scenario when one citation partially supports the statement. We discuss the details in § E .\n\n3.4 ALCE is Robust to Shortcut Cases\n\nWe showcase how the ALCE evaluation is robust to two possible shortcuts in § D : (1) using the top-1 retrieved passage as the response and citing itself, and (2) using the first two sentences of the top-1 passage. Both cases have almost-perfect citation scores, but (1) has low fluency due to its unnaturally long length compared to human answers, and (2) has low correctness due to low coverage.\n\n4 Modeling\n\nIn this section, we discuss three major modeling components for an ALCE system—retrieval, synthesis, and post-editing.\n\n4.1 Retrieval\n\nWe explore simple, off-the-shelf retrievers. We use dense retrievers for Wikipedia, including GTR ( Ni\n\net al. , 2022 ) and DPR ( Karpukhin et al. , 2020 ); we use BM25 for Sphere. For each question, we retrieve the top-100 passages.\n\n4.2 Synthesis\n\nWe focus on how to prompt an LLM to interact with the retriever, and synthesize and cite the evidence (without fine-tuning internal parameters). One noteworthy challenge is that existing LLMs all have limited context window and thus can only fit a handful of passages.\n\nV ANILLA . We simply provide the model with the topk 9 passages and instruct the model to cite accordingly (Table 2 ). We also use in-context learning ( Brown et al. , 2020 ) and prepend two demonstrations. The complete instruction is in Table 23 .\n\nS UMM /S NIPPET . With a 4K context window, we can at most safely fit k = 5 passages. As shown in Figure 4 , top-5 retrieved passages can only cover 56.8% percent of the answers in ASQA.\n\nTo tackle this limitation, we propose to provide summaries or snippets of passages instead of the full text (summaries are abstractive but snippets are spans from passages). We acquire summaries and snippets by prompting ChatGPT with instructions (prompts in Table 25 and 26 ). Then we replace all passages with summaries/snippets. Summaries or snippets significantly reduce the passage length, allowing for more passages to fit in: for ASQA, they reduce passage length by 6 × on average.\n\nThough S UMM /S NIPPET allows for more retrieved passages, they are lossy compressions. To alleviate this problem, we propose I NTERACT , an interactive prompting scheme to allow the model to check the full text of certain passages. At each step, the model can execute one of three actions: (1) “ Check: Document [1][2] ” to check the full text of the corresponding documents; (2) “ Output: ” to output a statement of the answer; (3) “ End. ” to end the generation. § C provides more details.\n\nI NLINE S EARCH . The above methods all display retrieval results at the beginning. In I NLI - NE S EARCH , we allow LLMs to call “search” during the generation process ( Yao et al. , 2023 ; Press et al. , 2022 ; Jiang et al. , 2023 ). At each step, the model can execute one of three actions: “ Search:\n\nInstruction: ...\n\nTable 3: An example of I NLINE S EARCH .\n\n{query} ” to search among the top-100 passages 11 by using GTR; the “ Output ” and “ End ” actions are the same as I NTERACT . For each “ Search ” action, we display the best retrieved passage in the context. The passage is removed after one action to save context space. Table 3 shows an example.\n\nC LOSED B OOK . We also add a simple closedbook baseline, where the model is only prompted with the instruction and the question, without any retrieved passages provided. Consequently, this variant does not cite any evidences.\n\n4.3 Post-editing\n\nIn this section we discuss two strategies for refining the output to further improve its quality.\n\nR ERANK . We randomly sample n sample = 4 responses for each question, and select the best response using the automatic citation recall score. we expect R ERANK to improve the citation quality.\n\nP OST C ITE . For each statement, we find the best matching passage among the top-100 retrieved passages using GTR and cite it. We combine this with C LOSED B OOK in our experiments.\n\n5 Experiments\n\nWe describe experiment details in § C . We use ChatGPT ( gpt-3.5-turbo-0301 ) with a 4K context window for most main experiments and ablations. We also report results with ChatGPT-16K ( gpt3.5-turbo-16k-0613 ) and GPT-4 ( gpt-4-0613 ; 8K context window). For open-source models, we test LLaMA ( Touvron et al. , 2023a ) and its instruction-tuned versions, including Alpaca ( Taori et al. , 2023 ), Vicuna ( Chiang et al. , 2023 ), and Table 4: Experiments on ASQA. For C LOSED B OOK , we use P OST C ITE to get citations. k -psg: putting topk passages from the retrieval results into the context. Chat-13B and Chat-70B refer to LLaMA-2-Chat.\n\nOasst ( Köpf et al. , 2023 ). They all have a 2K context window. We use short instructions for LLaMA (Table 24 ) to save context budget. Additionally, we test LLaMA-2-Chat, which were also trained to follow instructions ( Touvron et al. , 2023b ). These models have a context window of 4K tokens, which allows for 5 passages per question.\n\n5.1 Main Results\n\nWe present the main results on three datasets in Table 4 , 5 , and 6 respectively (full results in § G.6 ). We first note that all models achieve good fluency scores (except some models on ELI5 mainly due to their longer generations). We summarize the main takeaways from the experiments below.\n\nV ANILLA achieves strong performance. Despite its simplicity, V ANILLA (putting retrieved passages in context) achieves close-to-the-best performance among all prompting strategies.\n\nUsing summaries or snippets improves correctness. We see a universal trend that S UMM or S NIP - PET improves correctness, though on ASQA and ELI5, such an improvement comes at a cost of citation quality due to the lossy compression. Combining I NTERACT with S UMM /S NIPPET does not bring improvement, and we hypothesize that checking the full passages offers limited benefit and current LLMs are not proficient in an interactive usage.\n\nRetrieving text on the fly does not improve performance. All datasets show that V ANILLA outperforms I NLINE S EARCH on citation quality (and on correctness for ASQA and ELI5). By manually examining the examples, we find that it is challenging to ask detailed questions without seeing any passages. To improve I NLINE S EARCH , one may need to provide more context about the questions in advance or encourage the model to call retrievers with more detailed and diverse queries.\n\nTable 5: Experiments on QAMPARI. “Rec.-5”: we set the recall to be 100% if the prediction includes at least 5 correct answers.\n\nR ERANK boosts citation quality. We observe that R ERANK leads to consistent improvement in citation quality (on ASQA and ELI5). As the automatic scores may be biased in R ERANK , we also conduct human evaluation (§ 6 ) and verify its effectiveness.\n\nC LOSED B OOK +P OST C ITE delivers strong correctness but poor citation quality. C LOSED - B OOK outperforms V ANILLA in correctness on ELI5 and QAMPARI, and has only a 2% gap on ASQA. However, C LOSED B OOK cannot provide any citation; when combined with P OST C ITE , the citation quality remains inadequate. For instance, citation recall of C LOSED B OOK +P OST C ITE is lower than V ANILLA by 47% on ASQA.\n\nTo understand why C LOSED B OOK achieves better correctness and why P OST C ITE cannot deliver satisfying citation quality, we manually examine model outputs and find that: (1) open-book models are easily distracted by irrelevant passages and generate responses with lower correctness, a phenomenon also observed by Shi et al. ( 2023 ); (2) C LOSED B OOK often generates texts that are correct but not similar to any retrieved passages, making it difficult to match a citation post-hoc.\n\nTable 6: Experiments on ELI5. We use claim recall for the correctness evaluation. Chat-13B and Chat-70B refer to LLaMA-2-Chat.\n\nGPT-4 brings limited improvement but is better at using long context. We evaluate GPT-4 with V ANILLA and different numbers of passages (more results in § G.6 ). GPT-4 brings consistent (but limited) improvement on correctness, but often at a cost of citation quality. GPT-4 can also incorporate more passages due to its longer context window, which boosts both correctness and citation quality. On the contrary, including more passages with ChatGPT-16K does not improve the results (Table 7 ), suggesting that processing more passages is non-trivial and GPT-4 is better at synthesizing information from its long context than ChatGPT.\n\n5.2 Comparison of Different LLMs\n\nTable 7 compares different LLMs on ASQA using V ANILLA (more results in § G.6 ). Notably, instruction-tuned models (Vicuna-13B and LLaMA-2-Chat) outperform the original LLaMA models in correctness and considerably enhance the citation quality. We observe that while the original LLaMA models are able to copy facts from the context, they struggle with accurately citing the sources or simply do not cite. Notably, the best open-source model, LLaMA-2-70B-Chat, achieves comparable correctness score as the OpenAI models, but still lags behind in citation quality.\n\n5.3 Retrieval Analysis\n\nThe retrieval results play a crucial role to the correctness and the citation quality. Figure 4 presents the retrieval recall@ k with different datasets and Table 7: Comparison of different LLMs on ASQA (GTR+V ANILLA ). LLaMA-13B and Vicuna-13B have a context limit of 2,048 tokens, and thus can only use a short version of instructions and at most top-3 passages. Chat-13B and Chat-70B refer to LLaMA-2-Chat.\n\nFigure 4: Retrieval recall@ k on ASQA ( EM recall ), QAMPARI ( recall-5 ), and ELI5 ( claim recall ). Retrieval recall serves as an upper bound for model performance, and we compare them with two models’ correctness results in the figure (dashed lines): “Vanilla (5-psg)” is ChatGPT V ANILLA with top-5 passages in context; “Oracle” is the same model except that it uses 5 gold passages (§ G.1 ), whose recall matches Recall@100 on all three datasets.\n\nretrievers. As the number of passages increases, retrieval recall steadily improves. Additionally, Figure 4 shows the correctness performance of two models: (1) ChatGPT V ANILLA with top-5 passages (our primary baseline); (2) an oracle version of the same model employing 5 gold passages (§ G.1 ; the 5 gold passages match the retrieval recall@100). Notably, both models’ correctness lags behind the corresponding retrieval recall (except for ELI5 top-5). The discrepancy suggests that despite the presence of accurate answers in context, LLMs struggle to utilize them in their outputs.\n\nWe compare the impact of different retrievers and different numbers of passages to LLMs. Figure 4 (right) shows that GTR outperforms DPR in both correctness and citation quality, emphasizing the importance of deploying better retrievers. Contrary to the retrieval recall trend in Figure 4 , more passages in context do not yield substantial improvement for ChatGPT. Specifically, correctness plateaus at top-1 passage and citation quality plateaus at top-3. GPT-4 (Table 7 ) exhibits an increasing trend with more passages, but the improvement is not proportional to the retrieval performance. This indicates the limited ability of LLMs in utilizing multiple passages within context.\n\n5.4 Other Ablations\n\nWe provide additional ablations in § G . In summary, we find that (1) using comprehensive instructions enhances the citation quality of instruction-tuned models (§ G.2 ); (2) including at least one demonstration improves the performance (§ G.3 ); (3) finetuned models (FiD; Izacard and Grave , 2021 ) with P OST C ITE lag behind LLMs in both correctness and citation quality and fail to generalize (§ G.4 ).\n\n6 Human Evaluation\n\nTo verify that our automatic evaluation correlates with human judgement, we conduct human evaluation on selected models and request workers to judge model generations on three dimensions similar to Liu et al. (2023)—(1) utility: a 1-to-5 scoreindicating whether the generation helps answer the question; (2) citation recall: the annotator is given a sentence and all passages that the sentence cited, and is asked to judge whether the passages fully support the sentence; (3) citation precision: given a sentence and one of its citations, the annotator is asked to judge whether the citation “fully supports”, “partially supports”, or “does not support” the sentence. Each citation gets a precision score 1 if the output sentence has a citation recall of 1 and this citation at least “partially supports” it. See Appendix F for more details.\n\nModel outputs score high utility. The utility scores do not differ significantly between models, ranging 3 . 7 - 3 . 9 for ASQA and 3 . 5 - 3 . 6 for ELI5. Upon inspection, all tested models are mostly able Table 9: Human citation quality evaluation vs. ALCE citation quality evaluation on ELI5.\n\nTable 8: Human citation quality evaluation vs. ALCE citation quality evaluation on ASQA.\n\nto output fluent answers that are related to the question, despite differences in factual correctness.\n\nOur automatic evaluation of citation quality strongly correlates with human judgements. As shown in Table 8 (ASQA) and Table 9 (ELI5), the relative rankings induced by human and our automatic metrics are consistent. The absolute citation scores from human and ALCE are very close except for R ERANK (which uses the automated citation recall for reranking). This suggests that an improvement on ALCE citation metrics translates to improvement on human preferences. Furthermore, the Cohen’s kappa coefficient between human and ALCE suggests substantial agreement for citation recall ( 0 . 698 ) and moderate agreement for citation precision ( 0 . 525 ). We also show in § G.5 that our automatic evaluation achieves high accuracy when treating human annotations as gold labels ( 85 . 1% for citation recall and 77 . 6% for citation precision).\n\n7 Related Work\n\nEvaluating citations. Generating text with citations is closely related to attribution. Rashkin et al. ( 2023 ) define the “attributable to identified sources” (AIS) score to measure how faithful a generated text is to its sources. Bohnet et al. ( 2022 ) apply AIS scores on a single-document short-answer QA dataset. Honovich et al. ( 2022 ); Yue et al. ( 2023 ) study automatic evaluations for the AIS score. A concurrent work ( Liu et al. , 2023 ) conduct human evaluation on commercial generative search engines to examine their citation qualities.\n\nScientific citation text generation ( Funkquist et al. , 2022 ) is a related task to ALCE where the model is provided the papers-to-cite and context and is required to recover the citing text. It is different from ALCE as all citations are provided and the model only needs to perform the summarization.\n\nRetrieval-augmented LMs. Many studies have explored augmenting LMs with externally retrieved information. Guu et al. ( 2020 ); Borgeaud et al. ( 2022 ); Izacard et al. ( 2022 ) pre-train language models with retrieved passages, while Khandelwal et al. ( 2020 ); Zhong et al. ( 2022 ) augment LLMs’ output by interpolating it with a k NN module; though none of them explicitly provide citations to the retrieved sources. Other works prompt or fine-tune LLMs to “retrieve on-the-fly” ( Parisi et al. , 2022 ; Schick et al. , 2023 ; Shuster et al. , 2022 ; Jiang et al. , 2023 ; Yao et al. , 2023 ; Press et al. , 2022 ), which offers flexibility of when and what to search. Gao et al. ( 2023 ); He et al. ( 2022 ) propose to first generate text without accessing external documents and then retrieve relevant documents and revise the generation to be consistent.\n\nAmong previous explorations, Nakano et al. ( 2021 ); Menick et al. ( 2022 ) are the closest to our setting, where LLMs are trained to answer questions while providing citations. However, they do not explore retrieval strategies and simply use commercial search engines, which are not reproducible, and their models and training data are closedsource. To the best of our knowledge, we are the first to implement end-to-end systems that retrieve, synthesize, and cite documents with LLMs.\n\n8 Conclusion\n\nWe propose ALCE, the first automatic benchmark for evaluating LLM generations with citations. We deploy automatic metrics to measure fluency, correctness, and citation quality, and verify their efficacy via human evaluation. We explore a variety of strategies for incorporating citations in LLMs and demonstrate that current systems have considerable room for improvement on ALCE.\n\nOur experiments highlight a number of promising research directions, including (1) enhancing retrieval and refining retrieval integrations in LLMs, (2) developing long-context LLMs, and (3) advancing LLMs’ ability to synthesize multiple sources. What’s even more intriguing is that these research proposals extend beyond the ALCE setup (for example, long-context LLMs have numerous exciting applications), and ALCE can serve as a valuable testbed for their development.\n\nLimitations\n\nOur evaluation still has room for improvement: (1) MAUVE is found to be sensitive to output length and may provide unstable results; (2) for the ELI5’s correctness evaluation, the automatically generated claims may not cover all possible answers due to the open-ended nature of the questions; (3) our citation quality evaluation is limited by the accuracy of the NLI model; for citation precision, the NLI model cannot detect the case of “partially support” and thus leads to a lower citation precision score than the human evaluation.\n\nAlthough we believe our curated datasets closely resemble the distribution of real-world user questions, we acknowledge that they do not cover more challenging scenarios, such as multi-hop reasoning, math reasoning, and code completion.\n\nIn our experiments, we focus on prompting LLMs without updating their model weights. Training a model directly to incorporate citations remains challenging due to the lack of supervised data. However, we observe that certain humaninstruction datasets contain examples similar to our task setup. We leave the exploration of training LLMs to generate citations for future work.\n\nAcknowledgments\n\nWe appreciate the helpful feedback from the members of the Princeton NLP group. We thank Alexander Wettig, Nelson Liu, Tianyi Zhang, Yu Meng, Sadhika Malladi, Yangsibo Huang, Zhiyuan Zeng, and Dan Friedman for the valuable discussion. We thank Surge AI (especially Anna Folinsky and Edwin Chen) for their support with the human evaluation. Tianyu Gao is supported by an IBM PhD Fellowship. This research is supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, and Microsoft Azure credits through the “Accelerate Foundation Models Academic Research” Initiative."
}