[
    {
        "type": "text",
        "text": "Generation of Scientiﬁc Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS) ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Ruihua Qi 1,2( B ) , Weilong Li 1 , and Haobo Lyu 1 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1  School of Software Engineering, Dalian University of Foreign Languages, Dalian 116044, China { 236493401,236493404 } @student.dlufl.edu.cn 2  Research Center for Language Intelligence, Dalian University of Foreign Languages, Dalian 116044, China rhqi@dlufl.edu.cn",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract.  With the rapid increase in the number and speed of scientiﬁc publications, researchers face signiﬁcant time pressure when conducting literature reviews. This paper presents an automatic literature review generation method leveraging large language models (LLMs) and multiagent systems (MAS). By designing multiple agent roles, including reference parsing, analysis, generation, and integration agents-this method fully utilizes the natural language processing capabilities of LLMs and the collaborative strengths of MAS to produce high-quality literature reviews. In the NLPCC2024 evaluation task, our method excelled in multiple automatic evaluation metrics (such as SoftHeadingRecall and ROUGE) and manual evaluations, showcasing its great potential for practical applications. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Keywords:  Large Language Models  ·  Multi-Agent Systems  · Literature Review Generation  ·  Natural Language Processing  · Collaborative Generation ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "With the rapid increase in the number and speed of scientiﬁc publications, researchers face signiﬁcant time pressure when conducting literature reviews. According to Johnson et al. (2018) [ 1 ], as of August 2018, there were 33,100 active peer-reviewed English journals, publishing approximately 3 million papers annually, with an annual growth rate of about 5%. Scientiﬁc literature surveys are foundational in advancing scholarly understanding and innovation. They play a crucial role in academic research by providing a comprehensive synthesis of existing knowledge within a speciﬁc ﬁeld, enabling researchers to identify gaps, trends, and key ﬁndings, and guiding future research directions. By systematically evaluating and integrating previous studies, literature surveys contribute to the validation and reﬁnement of theoretical frameworks. These surveys enable researchers to gain a thorough understanding of existing work within their ﬁeld, uncover gaps in the literature, and discover potential directions for future research. By questioning established assumptions, pinpointing key issues, and stimulating scientiﬁc dialogue, literature surveys signiﬁcantly contribute to the progression of the discipline. They systematically analyze and synthesize the literature using various methods, such as critical evaluation, exploratory description, and integrative assessment. This process not only consolidates current knowledge but also suggests new pathways for advancing research. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Generating a scientiﬁc literature survey requires meticulous planning, extensive searching, and rigorous screening of a large body of literature. This task involves systematically reviewing and synthesizing existing research within a speciﬁed domain. It requires identifying, evaluating, and integrating relevant studies to provide a comprehensive overview of the current state of knowledge [ 2 ]. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Traditional methods for conducting literature reviews primarily rely on manual searching, reading, and synthesizing literature. Although this approach is thorough and comprehensive, it has signiﬁcant drawbacks. First, manually processing a large volume of literature requires substantial time and eﬀort [ 3 ]. Second, due to personal biases and knowledge gaps, researchers may overlook important studies when selecting and evaluating literature [ 4 ]. As the volume of scientiﬁc publications rapidly grows, this process becomes increasingly complex, and accurately synthesizing and interpreting collected data becomes more time-consuming and labor-intensive. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In recent years, advancements in artiﬁcial intelligence, particularly natural language processing (NLP) technologies, have provided new tools to enhance the eﬃciency of literature reviews [ 5 ]. The emergence of large language models (LLMs) oﬀers new opportunities for automating and streamlining the systematic literature review (SLR) process. LLMs are trained on extensive text datasets and excel at understanding and generating human-like language [ 6 ]. Research has shown that LLMs perform exceptionally well in understanding context, identifying key points, and generating summaries, making them highly eﬀective in assisting literature reviews [ 7 ]. They can quickly process and analyze large volumes of text, generating structured summaries and analyses, signiﬁcantly improving the eﬃciency of literature reviews. Multi-agent systems (MAS) further enhance this process by facilitating distributed problem-solving and parallel processing, thereby improving the eﬃciency and accuracy of handling extensive literature. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "This paper aims to enhance the eﬃciency and quality of scientiﬁc literature surveys by leveraging large language models (LLMs) and multi-agent systems (MAS). By systematically integrating natural language processing technologies, this research addresses inherent challenges in traditional literature review processes, such as time consumption, potential biases, and overwhelming volume of literature. The main contributions are as follows: ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1. Automated scientiﬁc literature survey generation with LLMs: Utilizing LLMs to automatically generate literature surveys, improving coherence and accuracy.   \n2. Design of Multi-Agent System for automated scientiﬁc literature survey generation, implementing MAS to integrate diverse methods and perspectives, enriching the analytical depth of literature surveys. This approach enhances eﬃciency and scalability through collaborative task division, and provides robust error detection and correction capabilities. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2 Related Work ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.1 Automation of Systematic Literature Reviews ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "According to Ananiadou et al. (2009) [ 8 ], literature reviews involve searching for relevant studies, screening to focus on key evidence, mapping research activities to engage stakeholders and set priorities, and synthesizing evidence from diverse sources. Preparing a systematic review includes creative tasks for developing questions and protocols, and technical tasks that can be automated (Tsafnat et al., 2014) [ 9 ]. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In the current landscape of systematic literature review (SLR) automation, natural language processing (NLP) preprocessing involves various techniques. Traditional SLR automation uses shallow machine learning methods, requiring manual feature engineering and ﬁne-tuning for each speciﬁc domain or dataset. Many legacy NLP representation techniques are now replaced by word embeddings, which capture the meaning of words and reduce the need for extensive text cleaning and manual feature creation. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "SLR automation is typically achieved through classiﬁcation, a supervised machine learning task, with models commonly evaluated using cross-validation. Key metrics include Work Saved over Sampling (WSS), Recall, Precision, and Fmeasure, with WSS encompassing the essence of the latter three metrics, making it the preferred metric for SLR automation. Support Vector Machines (SVM) and Bayesian Networks, such as Na¨ ıve Bayes classiﬁers, are predominantly used across various steps of the SLR process. However, there is a signiﬁcant lack of evidence supporting the use of deep learning techniques in SLR automation [ 10 ]. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Van Dinter et al. (2021) [ 10 ] conducted a systematic review of literature related to literature reviews to collect and summarize the current state of research. Their study is the ﬁrst systematic literature review focusing on automated systems for literature reviews, emphasizing all stages of the review process, including NLP and ML techniques from retrospective methods. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.2 Automatic Literature Review Generation Using LLMs ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Automatic literature review generation is a complex challenge in Natural Language Processing (NLP). Traditional methods are time-consuming and laborintensive, often biased and incomplete due to the vast amount of information. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Recent advancements in Large Language Models (LLMs) have shown signiﬁcant potential in automating this process. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Kasanishi et al. (2023) [ 11 ] introduced SciReviewGen, a large-scale dataset for automatic literature review generation, containing over 10,000 literature reviews and 690,000 cited papers, primarily in computer science. This dataset facilitates the evaluation of transformer-based summarization models for literature reviews. They also proposed the Query-weighted Fusion-in-Decoder (QFiD) model, which weights cited papers based on query relevance. Experiments showed that QFiD generates more relevant and coherent reviews than other models, though challenges like hallucinations and insuﬃcient detail remain. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The introduction of SciReviewGen and QFiD underscores LLMs’ potential to enhance literature review eﬃciency and reduce manual eﬀort. This dataset and model lay the groundwork for future research in automatic literature review generation, addressing issues like citation network integration and full-text information enhancement to improve review accuracy and comprehensiveness. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.3 Utilizing LLMs to Improve Literature Review Eﬃciency ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Antu et al. (2023) [ 12 ] explored using LLMs like OpenAI’s ChatGPT to help students complete literature reviews for their theses. They found that ChatGPT signiﬁcantly enhances review eﬃciency and comprehensiveness by quickly processing large volumes of text and providing structured summaries. However, limitations include generating non-existent citations (hallucinations) and using outdated or irrelevant sources. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "In case studies in computer science and communication, Antu et al. demonstrated ChatGPT’s ability to generate research topics and identify relevant literature [ 12 ]. Despite drawbacks such as overgeneralization and inaccuracy, ChatGPT serves as a useful starting point for literature reviews. The authors emphasized the importance of “human-AI collaboration,” where researchers critically assess and reﬁne AI-generated outputs to ensure quality and relevance. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The study shows that while LLMs like ChatGPT can streamline the literature review process, human oversight is crucial. This combined approach of AI and human expertise helps undergraduates complete their research more eﬀectively, balancing the strengths of both. The comparative analysis highlights the transformative potential of LLMs in academic research, especially in automating tedious literature review tasks. However, both studies stress the importance of human supervision to mitigate AI-generated content’s limitations, such as hallucinations and biases [ 13 ]. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 Methodology ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.1 Task Deﬁnition ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The objective of the Scientiﬁc Literature Survey Generation competition in NLPCC 2024 is to develop a retrieval-enhanced text generation model capable of producing comprehensive literature reviews. A series of topic-related references and provided metadata (including titles, topics, abstracts, and references) are utilized to generate high-quality literature reviews. The tasks involve training and ﬁne-tuning the model using the supplied training and validation sets. The goal is to create a model that produces literature reviews meeting speciﬁc evaluation criteria. In the ﬁnal stage, literature reviews are generated based on the test dataset, and the resulting texts generated by the model are submitted for assessment. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.2 Motivation ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "In the scientiﬁc research ﬁeld, the rapid increase in the number and speed of publications imposes signiﬁcant time and workload pressures on researchers conducting literature reviews. Traditional manual review methods are time-consuming and labor-intensive, hindering the ability to quickly and eﬃciently cover the latest research. Review papers typically follow a structured format: title, abstract, main text, and conclusion. This paper introduces an automated literature review generation method leveraging large language models (LLM) and multi-agent systems (MAS) to address this issue. Our system is especially suited for ﬁelds like computer science and life sciences that require rapid processing and summarization of extensive literature. The design aims to enhance review eﬃciency and quality by intelligently parsing, analyzing, generating, and integrating literature, distributing the review process among various LLM-based agents. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "To tackle these challenges, we designed a multi-agent system where each agent is responsible for tasks such as literature parsing, analysis, generation, and integration. These agents collaborate, utilizing the robust text generation capabilities of large language models to automatically produce structured, highquality literature reviews. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Our approach is grounded in the theories of natural language processing and multi-agent systems. Large language models, like the qwen [ 14 ] series from Tongyi Qianwen and OpenAI’s GPT [ 15 ], excel in text generation and understanding, producing coherent and high-quality text. For this experiment, we employed the qwen-long [ 16 ] model, known for its strong generation capabilities, cost-eﬀectiveness, and suitability for large-scale applications. Multi-agent systems manage complex tasks eﬃciently through division of labor and collaboration. By combining these technologies, we achieve eﬃcient and eﬀective automatic literature review generation. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.3 The Proposed Model ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The system introduced in this study, named the “Automated Literature Review Generation System” (ALRGS), is depicted in Fig.  1 . The system consists of ﬁve main agents: the Reference Analysis Agent, the Title & Abstract Writer Agent, the Section Content Writer Agent, the Conclusion Writer Agent, and the Integrator Agent. Each agent is responsible for a speciﬁc task, and through their collaborative eﬀorts, they enable eﬃcient and automated literature review generation. Below is a detailed description of each agent. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/8191e6a6d8c2bcd883646339bc219557350aa1f60a4d38e929ed966a87ed13d4.jpg",
        "img_caption": [
            "Fig. 1.  Workﬂow of the ALRGS. "
        ],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Each of these agents performs its speciﬁc task, utilizing predeﬁned workﬂows to achieve eﬃcient and automated literature review generation. The following section provides a detailed description of each agent and its core algorithm. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Reference Analysis Agent.  The primary function of the Reference Analysis Agent is to extract references and related content from the given JSON ﬁle. Using predeﬁned prompts, this agent analyzes the references, identiﬁes their logical relationships within the review, and classiﬁes them. The classiﬁcation results are presented as structured chapter titles and related reference lists, ensuring that each reference is individually listed and described, avoiding any merging or summarizing. The prompt used by this agent is shown in  Prompt1 . ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Prompt1:Reference Analysis Agent ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Please analyze the following references and their abstracts, identify their logical relationships in the review, and classify them. All references must be listed individually, without omission or merging. Output structured section titles and list the related references under each section title. Ensure that each reference is clearly associated with a speciﬁc section, and avoid summarizing multiple references together. The output must be organized, with accurate and concise section titles that reﬂect the thematic grouping of the references. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Title and Abstract Writer Agent.  This agent aims to automatically generate high-quality titles and abstracts based on the analysis of structured references. By reading classiﬁed references from the speciﬁed ﬁle, the agent identiﬁes themes and logical relationships, generating suitable titles and concise abstracts for the review paper. This process leverages the robust text understanding and logical reasoning capabilities of large language models to ensure academic accuracy. The prompt used by this agent is shown in  Prompt2 . ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Prompt2:Title and Abstract Writer Agent ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Based on the following section titles, please generate an appropriate title and a brief abstract for a review paper: Section Titles: Title: “title here” Abstract: “abstract here”. Please generate the paper title and abstract. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Section Content Writer Agent.  The Section Content Writer Agent generates the main text content for each section by leveraging the structured and classiﬁed reference texts. Utilizing the text generation capabilities of large models, it ﬁnetunes LLM parameters (Prompt Engineering) and employs generation strategies to produce high-quality review content that adheres to academic standards. The prompt used by this agent is shown in  Prompt3 . ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Prompt3:Section Content Writer Agent ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The following are section titles and related references. Please write the text content for each section. “structured references” Please write the section content. Only the body text needs to be outputted, omitting all other parts. Do not include #, and do not reply in Markdown format. Respond as per the speciﬁed requirements. Ensure that each section’s content is suﬃciently detailed and provides comprehensive analysis. The output should be at least 3000 characters long. Without hashtag #. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Conclusion Writer Agent.  This agent receives the section content generated by the previous agent and systematically organizes and analyzes the review content. It identiﬁes and integrates key points and research ﬁndings from each section to create a precise and comprehensive conclusion. The Conclusion Writer Agent synthesizes these points into a coherent and logical conclusion, ensuring the review’s completeness and academic integrity. The prompt used by this agent is shown in  Prompt4 ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Prompt4:Conclusion Writer Agent ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Based on the following section contents, please write the Conclusion section for the review paper. Section Contents: “chapter contents” Do not include #. Please write the Conclusion: ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Integrator Agent.  This agent compiles the content generated for each section into a complete literature review and produces the ﬁnal document in XML format. By outputting in XML, the system ensures the review is structured and standardized, facilitating further evaluation and publication. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In summary, our automated literature review generation system, based on LLM and MAS, achieves eﬃcient and accurate literature review generation through intelligent parsing, analysis, generation, and integration. This signiﬁ- cantly reduces researchers’ workload and enhances the coverage and accuracy of literature reviews. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4 Experiments ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4.1 Data ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In this experiment, we used a dataset of 500 English scientiﬁc review papers, with 400 for the training set and 100 for the validation set [ 17 ]. Each sample includes the article’s title, article id, subject, abstract, references, reference content, and review content. The training data samples are provided in JSON format, with “reference content” containing the titles and abstracts of some references. The test dataset consists of 200 review papers, formatted as JSON ﬁles with the structure missing reference abstracts through web scraping to ensure completeness and  { “subject”: “”, “reference”: [...] } . All data were supplemented with quality. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4.2 Experimental Setup ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In this experiment, we employed multiple large language models (qwen-long, gpt4o [ 18 ], gpt-3.5-turbo [ 19 ]) to generate literature reviews. All models utilized the dataset supplemented with reference abstracts through web scraping, ensuring more eﬀective use of reference content during the generation process. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4.3 Evaluation Metrics ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "To evaluate the quality of the generated literature reviews, we adopted multiple evaluation metrics, as shown in ( 1 )-( 5 ), including automated and structural assessments: ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "–  Automated Evaluation : We used ROUGE-1/2/L to assess the content quality of the generated reviews [ 20 ]. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "–  Structural Evaluation : We employed Soft Heading Recall to evaluate the structure of the generated reviews, embedding all generated chapter titles using the bge-large-en-v1.53 model. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "–  Manual Evaluation : LLMs and human experts evaluated the following aspects: language ﬂuency and clarity; logical structure; suﬃciency, reliability, and accuracy of citations; consistency with the theme, ensuring no deviation; and comprehensiveness of the analysis. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "4.4 Experimental Results and Analysis ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "In this experiment, we compared the eﬀectiveness of the qwen-long model with the combined use of gpt-4o and gpt-3.5-turbo. The results of this comparison are summarized in Table  1 , which presents the performance of the models across various evaluation metrics, including Soft Heading Recall and ROUGE scores. Analyzing these results, we derived the following key conclusions. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Firstly, as shown in Table  1 , the qwen-long model excelled in the Soft Heading Recall metric, demonstrating high accuracy and consistency in generating chapter titles and content structures for reviews. This advantage is largely due to the qwen-long model’s excellent performance in handling long texts and complex contextual relationships. Its strong contextual understanding ensures logically coherent and well-structured content. ",
        "page_idx": 8
    },
    {
        "type": "table",
        "img_path": "images/f5bf3e2fac9e682d4f13563def4629bcc4a55da70aa0d69a6369297022242424.jpg",
        "table_caption": [
            "Table 1.  Experiments Result. "
        ],
        "table_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Additionally, the qwen-long model’s training data spans a wide range of ﬁelds and topics, providing it with strong generalization capabilities. This allows the model to adapt ﬂexibly to various themes and types of literature. The broad adaptability and low computational cost make the qwen-long model highly costeﬀective for large-scale applications, especially in research ﬁelds such as computer science and life sciences that require processing large volumes of literature. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "On the other hand, although the qwen-long model excels in structure generation, the combination model of gpt-4o and gpt-3.5-turbo slightly outperforms it in the details and diversity of content generation. This advantage is reﬂected in the combination model of ChatGPT, which, by integrating diﬀerent versions of models and leveraging their strengths, enhances the diversity and detail processing capabilities of the generated text. The synergistic eﬀect of multiple models makes the generated content richer and more vivid, better matching the vocabulary and phrases of the references, thereby performing excellently on ROUGE-1 and ROUGE-2 metrics. This diverse expression and detailed content generation capability make the ChatGPT combination model perform better in tasks requiring high-precision content generation. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Nevertheless, each model has its applicable scenarios. The qwen-long model, while ensuring generation quality, has a higher cost-eﬀectiveness, suitable for tasks that require rapid processing and summarization of large volumes of literature. Its advantage in structure generation allows it to provide high-quality chapter divisions and content organization. The ChatGPT combination model, on the other hand, excels in the details and diversity of content generation, making it suitable for tasks that require high-precision content generation. For example, for literature review tasks that need detailed analysis and description, the ChatGPT combination model can provide richer and more detailed content. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Additionally, our system design features a signiﬁcant advantage with its highly decoupled, independent agents. This allows the selection of the most suitable model and agent based on the speciﬁc task’s diﬃculty and requirements. This ﬂexibility enhances the system’s applicability and scalability, enabling dynamic adjustment and optimization of model usage for diﬀerent types of literature reviews. For example, for tasks requiring rapid large-scale literature structure generation, the qwen-long model is preferred; for tasks needing high-detail content generation, the ChatGPT combination model is used. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "In summary, this study provides new insights and methods for automated literature review generation by comparing the performance of diﬀerent models, oﬀering signiﬁcant application value and research implications. Future research will continue to explore combining and optimizing the strengths of diﬀerent models to further improve the performance and practical application of literature review generation systems. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "5 Conclusion ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "This paper presents a method that combines Multi-Agent Systems (MAS) and Large Language Models (LLMs) to enhance the eﬃciency and quality of literature reviews. By breaking down the task into multiple subtasks and assigning speciﬁc responsibilities to each agent, we optimize parameter settings, and improve process interpretability. This method not only addresses traditional issues such as time consumption, bias, and information overload but also showcases the potential of LLMs in academic research. The main contribution of this paper is the introduction of a more eﬃcient and reliable literature review method, providing researchers with a new tool to improve the comprehensiveness and accuracy of their reviews. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Despite its many advantages, the method has certain limitations. First, LLMgenerated summaries of complex concepts may still be inaccurate. Second, the method’s eﬀectiveness relies on the quality and diversity of the training data. Future research directions include incorporating Retrieval-Augmented Generation (RAG) technology, allowing LLMs to automatically retrieve full texts based on titles and abstracts, thus reducing hallucinations. Additionally, future agent systems could evolve independently, dynamically optimizing their performance by adjusting goals and strategies based on both historical data and current needs. These improvements are expected to further enhance the quality of literature reviews, providing stronger support for academic research. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In conclusion, this study provides an eﬃcient and reliable method for literature reviews and suggests new directions for future research. We hope this method oﬀers valuable insights, fostering further development and optimization of literature review techniques. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Acknowledgement.  We would like to express our sincere gratitude to Kexin Technology for providing the invaluable data support and assistance for this event. Their contribution signiﬁcantly enhanced the quality and scope of our study. This work is partially supported by grant from the Applied Basic Research Project of Liaoning Province (No. 2022JH2/101300270); Project of Humanities and Social Sciences Research of the Ministry of Education (No.23YJAZH109); Promote scientiﬁc research cooperation with Canada, Australia, New Zealand and Latin America and High-level Talent Training Program (LiuJin Mei 2023 No.25). ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "1. Johnson, R., Watkinson, A., Mabe, M., Ware, M.: The STM Report: an overview of scientiﬁc and scholarly publishing. Technical and Medical Publishers, International Association of Scientiﬁc (2018) 2. Kraus, S., Breier, M., Lim, W.M., et al.: Literature reviews as independent studies: guidelines for academic practice. RMS  16 (8), 2577–2595 (2022) 3. Boote, D.N., Beile, P.: Scholars before researchers: on the centrality of the dissertation literature review in research preparation. Educ. Res.  34 (6), 3–15 (2005) 4. Ridley, D.: The literature review: a step-by-step guide for students (2012) 5. da Silva J´ unior, E.M., Dutra, M.L.: A roadmap toward the automatic composition of systematic literature reviews. Iberoamerican J. Sci. Meas. Commun. (2021) 6. Carlini, N., et al.: Extracting training data from large language models. In: 30th USENIX Security Symposium (USENIX Security 21), pp. 2633–2650 (2021) 7. Hariri, W.: Unlocking the potential of ChatGPT: a comprehensive exploration of its applications, advantages, limitations, and future directions in natural language processing. arXiv preprint learning, commerce, and politics. Wiley 8. Ananiadou, S., et al.: Supporting systematic reviews using text mining. Soc. Sci. Comput. Rev.  27 (4), 509–523 (2009) 9. Tsafnat, G., et al.: Systematic review automation technologies. Syst. Control Found. Appl.  3 , 1–15 (2014)   \n10. Van Dinter, R., Tekinerdogan, B., Catal, C.: Automation of systematic literature reviews: a systematic literature review. Inf. Softw. Technol.  136 , 106589 (2021)   \n11. Kasanishi, T., Isonuma, M., Mori, J., Sakata, I.: SciReviewGen: a large-scale dataset for automatic literature review generation. In: Findings of the Associa- tion for Computational Linguistics: ACL 2023, pp. 6695–6715. Toronto, Canada. Association for Computational Linguistics (2023)   \n12. Antu, S.A., Chen, H., Richards, C.K.: Using LLM (large language model) to improve eﬃciency in literature review for undergraduate research. LLM@ AIED, 8–16 (2023)   \n13. Williamson, S.M., Prybutok, V.: The era of artiﬁcial intelligence deception: unraveling the complexities of false realities and emerging threats of misinformation. Information  15 , 299 (2024).  https://doi.org/10.3390/info15060299   \n14. Bai, J., Bai, S., Chu, Y., et al.: Qwen technical report. arXiv preprint arXiv:2309.16609  (2023)   \n15. Brown, T.B., et al.: Language models are few-shot learners. In: Advances in Neural Information Processing Systems (NeurIPS).  https://arxiv.org/abs/2005.14165   \n16. Alibaba DAMO Academy.: Qwen-Long: Large Language Model.  https://damo. alibaba.com/technology/large-language-model/qwen-long   \n17. Yangjie, T., et al.: Overview of the NLPCC2024 Shared Task6: scientiﬁc literature survey generation.  http://tcci.ccf.org.cn/conference/2024/taskdata.php   \n18. OpenAI.: Hello GPT-4o. OpenAI.  https://openai.com/index/hello-gpt-4o/   \n19. OpenAI.: GPT-3.5 Turbo Overview. OpenAI.  https://platform.openai.com/docs/ models/gpt-3-5   \n20. Lin, C-Y.: Rouge: a package for automatic evaluation of summaries. Text summarization branches out, 74–81 (2004) ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    }
]